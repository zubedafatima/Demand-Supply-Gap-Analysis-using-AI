{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "#loading test weather data\n",
    "# List all the weather data files\n",
    "file_pattern = 'test_set\\weather_data\\weather_data_*'\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# Read the files and store them in a list of DataFrames\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, sep='\\t', header=None, names=['timestamp', 'weather', 'temperature', 'pollution'])\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all the DataFrames into a single DataFrame\n",
    "test_weather_data=None\n",
    "if dataframes:\n",
    "    test_weather_data = pd.concat(dataframes)\n",
    "else:\n",
    "    exit('No data files found.')\n",
    "\n",
    "# Convert the 'timestamp' column to a datetime object\n",
    "test_weather_data['timestamp'] = pd.to_datetime(test_weather_data['timestamp'])\n",
    "\n",
    "# Reset the index and sort the DataFrame by the timestamp\n",
    "test_weather_data = test_weather_data.sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "print(test_weather_data.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the training weather data\n",
    "file_pattern = 'training_data\\\\weather_data\\\\weather_data_*'\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# Read the files and store them in a list of DataFrames\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, sep='\\t', header=None, names=['timestamp', 'weather', 'temperature', 'pollution'])\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all the DataFrames into a single DataFrame\n",
    "train_weather_data=None\n",
    "if dataframes:\n",
    "    train_weather_data = pd.concat(dataframes)\n",
    "else:\n",
    "    exit('No data files found.')\n",
    "\n",
    "# Convert the 'timestamp' column to a datetime object\n",
    "train_weather_data['timestamp'] = pd.to_datetime(train_weather_data['timestamp'])\n",
    "\n",
    "# Reset the index and sort the DataFrame by the timestamp\n",
    "train_weather_data = train_weather_data.sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "print(train_weather_data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "# loading the test order data\n",
    "# List all the order data files\n",
    "file_pattern = 'test_set/order_data/test_order_data_*'\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# Read the files and store them in a list of DataFrames\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, header=None, names=['order_id', 'driver_id', 'passenger_id', 'start_region_hash', 'Time'])\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all the DataFrames into a single DataFrame\n",
    "test_order_data = None\n",
    "if dataframes:\n",
    "    test_order_data = pd.concat(dataframes)\n",
    "else:\n",
    "    exit('No data files found.')\n",
    "\n",
    "# Convert the 'Time' column to a datetime object\n",
    "test_order_data['Time'] = pd.to_datetime(test_order_data['Time'])\n",
    "\n",
    "# Reset the index and sort the DataFrame by the timestamp\n",
    "test_order_data = test_order_data.sort_values(by='Time').reset_index(drop=True)\n",
    "\n",
    "print(test_order_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the training order data\n",
    "# read training data\n",
    "# List all the training data files\n",
    "file_pattern = 'training_data\\\\order_data\\\\order_data_*'\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# Read the files and store them in a list of DataFrames\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, header=None, sep='\\t', names=['order_id', 'driver_id', 'passenger_id', 'start_region_hash', 'dest_region_hash', 'Price', 'Time'])\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all the DataFrames into a single DataFrame\n",
    "training_order_data = None\n",
    "if dataframes:\n",
    "    training_order_data = pd.concat(dataframes)\n",
    "else:\n",
    "    exit('No data files found.')\n",
    "\n",
    "# Convert the 'timestamp' column to a datetime object\n",
    "training_order_data['Time'] = pd.to_datetime(training_order_data['Time'])\n",
    "\n",
    "# Reset the index and sort the DataFrame by the timestamp\n",
    "training_order_data = training_order_data.sort_values(by='Time').reset_index(drop=True)\n",
    "\n",
    "print(training_order_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test cluster map\n",
    "test_cluster_map = pd.read_csv('test_set\\\\cluster_map\\\\cluster_map', sep='\\t', header=None, names=['region_hash', 'cluster_id'])\n",
    "print(test_cluster_map.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training cluster map\n",
    "train_cluster_map = pd.read_csv('training_data\\\\cluster_map\\\\cluster_map', sep='\\t', header=None, names=['region_hash', 'cluster_id'])\n",
    "print(train_cluster_map.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load test poi data\n",
    "def read_file(filename):\n",
    "    region_data = {}\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            items = line.strip().split('\\t')\n",
    "            region_hash = items[0]\n",
    "            values = items[1:]\n",
    "            region_data[region_hash] = values\n",
    "    return region_data\n",
    "\n",
    "test_poi_data = read_file('test_set\\\\poi_data\\\\poi_data')\n",
    "print(test_poi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training poi data\n",
    "train_poi_data = read_file('training_data\\\\poi_data\\\\poi_data')\n",
    "print(train_poi_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #-------------------Dummy data\n",
    "\n",
    "# orderdummy= pd.read_csv('D:\\Python-Project\\AI\\dummyorder.txt', delimiter='\\t')\n",
    "# weatherdummy= pd.read_csv('D:\\Python-Project\\AI\\dummyweather.txt', delimiter='\\t')\n",
    "\n",
    "# merged_df = pd.merge(weatherdummy, orderdummy, left_on='timestamp', right_on='Time',how='outer')\n",
    "# merged_df = merged_df.drop(columns=['timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_category(value):\n",
    "  num = value.split(':')[0]\n",
    "  #-------Storing that value in our set\n",
    "    #print(num)\n",
    "  if('#' in num):\n",
    "      val=num.split('#')                  #-------getting the sub category only instead of main\n",
    "      val=(val[1])\n",
    "  else:\n",
    "      val=num\n",
    "  \n",
    "  return val\n",
    "\n",
    "def split_count(value):\n",
    "  num = value.split(':')[1]\n",
    "  return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df=training_order_data.copy(deep=True)\n",
    "region_count={}\n",
    "#-----------------------counting the total facilities in each region\n",
    "for key, values in train_poi_data.items():      \n",
    "        count=0\n",
    "#------Looping over the list of facilities for a specific region\n",
    "        for value in values:\n",
    "        #-------splitting on semi colon to geth the facility type instead of the number of facility\n",
    "                val=split_count(value)\n",
    "                count=int(val)+count\n",
    "        #print(\"Region: count: \",key,count)\n",
    "        region_count.update({key: count})\n",
    "\n",
    "merged_df['Facilities'] = merged_df['start_region_hash'].map(region_count)\n",
    "print(merged_df)\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#---------merging the merged data with cluster map to replace the region hash value with its cluster ID value\n",
    "final = pd.merge(merged_df, train_cluster_map, left_on='start_region_hash', right_on='region_hash')\n",
    "final = final.drop(columns=['region_hash','start_region_hash'])\n",
    "final = final.rename(columns={'cluster_id': 'start_region_id'})\n",
    "\n",
    "final = pd.merge(final, train_cluster_map, left_on='dest_region_hash', right_on='region_hash')\n",
    "final = final.drop(columns=['region_hash','dest_region_hash'])\n",
    "final = final.rename(columns={'cluster_id': 'dest_region_id'})\n",
    "\n",
    "\n",
    "#---------extracting day of the week from the given time stamp\n",
    "final['Time']=pd.to_datetime(final['Time'])\n",
    "final['day']=(final['Time'].dt.day_name())\n",
    "\n",
    "# print(\"column count: \",final.shape[1])\n",
    "# print(\"row count\",final.count())\n",
    "#print(final.head())\n",
    "\n",
    "#-------------Sorting the table first according to region then within those regions, sorting according to time \n",
    "final = final.sort_values(by=['start_region_id', 'Time'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        start_region_id                Time  order_id  driver_id  Facilities  \\\n",
      "0                     1 2016-01-01 00:00:00       161        153    653376.0   \n",
      "1                     1 2016-01-01 00:10:00       173        166    653376.0   \n",
      "2                     1 2016-01-01 00:20:00       153        146    653376.0   \n",
      "3                     1 2016-01-01 00:30:00       137        133    653376.0   \n",
      "4                     1 2016-01-01 00:40:00       125        124    653376.0   \n",
      "...                 ...                 ...       ...        ...         ...   \n",
      "157166               66 2016-01-21 23:10:00         2          2    138942.0   \n",
      "157167               66 2016-01-21 23:20:00         1          1    138942.0   \n",
      "157168               66 2016-01-21 23:30:00         2          2    138942.0   \n",
      "157169               66 2016-01-21 23:40:00         4          3    138942.0   \n",
      "157170               66 2016-01-21 23:50:00         3          3    138942.0   \n",
      "\n",
      "        Gap  \n",
      "0         8  \n",
      "1         7  \n",
      "2         7  \n",
      "3         4  \n",
      "4         1  \n",
      "...     ...  \n",
      "157166    0  \n",
      "157167    0  \n",
      "157168    0  \n",
      "157169    1  \n",
      "157170    0  \n",
      "\n",
      "[157171 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "#----------------------Finding the Demand-supply gap for each ten minutes\n",
    "#print(sorted_final)\n",
    "\n",
    "#Group by Region and time in 10 minute intervals\n",
    "training = final.groupby(['start_region_id', pd.Grouper(key='Time', freq='10min')]).agg({'order_id': 'count', 'driver_id': lambda x: x.notnull().sum(),'Facilities':'mean'}).reset_index()\n",
    "\n",
    "#calculating the demand-gap by finding difference between total orders and given drivers\n",
    "training['Gap'] = training.apply(lambda row: (row['order_id'] - row['driver_id']), axis=1)\n",
    "print(training)\n",
    "\n",
    "#------counting how many rows of each region\n",
    "counts = training['start_region_id'].value_counts()\n",
    "counts_df = counts.reset_index().rename(columns={'index': 'region'})\n",
    "counts_df=counts_df.sort_values(by='region',ascending=True)\n",
    "print(counts_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      weather           timestamp  pollution  temperature\n",
      "0           1 2016-01-01 00:00:00      177.0          3.5\n",
      "1           1 2016-01-01 00:10:00      177.0          3.0\n",
      "2           1 2016-01-01 00:20:00      177.0          3.0\n",
      "3           1 2016-01-01 00:30:00      177.0          3.0\n",
      "4           1 2016-01-01 00:40:00      177.0          3.0\n",
      "...       ...                 ...        ...          ...\n",
      "2740        9 2016-01-18 14:50:00      276.0          6.0\n",
      "2741        9 2016-01-18 15:00:00      276.0          6.0\n",
      "2742        9 2016-01-18 15:20:00      276.0          5.0\n",
      "2743        9 2016-01-18 15:30:00      276.0          6.0\n",
      "2744        9 2016-01-18 15:40:00      263.0          5.0\n",
      "\n",
      "[2745 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#Grouping weather data into 10 min slots as well\n",
    "\n",
    "weather = train_weather_data.groupby(['weather', pd.Grouper(key='timestamp', freq='10min')]).agg({'pollution': 'mean','temperature':'mean',}).reset_index()\n",
    "print(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        weather  pollution  temperature  start_region_id                Time  \\\n",
      "0           1.0      177.0          3.5                1 2016-01-01 00:00:00   \n",
      "1           1.0      177.0          3.0                1 2016-01-01 00:10:00   \n",
      "2           1.0      177.0          3.0                1 2016-01-01 00:20:00   \n",
      "3           1.0      177.0          3.0                1 2016-01-01 00:30:00   \n",
      "4           1.0      177.0          3.0                1 2016-01-01 00:40:00   \n",
      "...         ...        ...          ...              ...                 ...   \n",
      "161145      2.0       58.0          1.0               66 2016-01-21 23:10:00   \n",
      "161146      2.0       58.0          1.0               66 2016-01-21 23:20:00   \n",
      "161147      2.0       58.0          1.0               66 2016-01-21 23:30:00   \n",
      "161148      2.0       59.0          1.0               66 2016-01-21 23:40:00   \n",
      "161149      2.0       59.0          1.0               66 2016-01-21 23:50:00   \n",
      "\n",
      "        order_id  driver_id  Facilities  Gap  \n",
      "0            161        153    653376.0    8  \n",
      "1            173        166    653376.0    7  \n",
      "2            153        146    653376.0    7  \n",
      "3            137        133    653376.0    4  \n",
      "4            125        124    653376.0    1  \n",
      "...          ...        ...         ...  ...  \n",
      "161145         2          2    138942.0    0  \n",
      "161146         1          1    138942.0    0  \n",
      "161147         2          2    138942.0    0  \n",
      "161148         4          3    138942.0    1  \n",
      "161149         3          3    138942.0    0  \n",
      "\n",
      "[161150 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "#---------merging weather data and order data on time\n",
    "merged_df = pd.merge(weather, training, left_on='timestamp', right_on='Time',how='right')\n",
    "merged_df = merged_df.drop(columns=['timestamp'])\n",
    "\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------ignore\n",
    "\n",
    "# df_sorted = final.sort_values(by=['start_region_id'], ascending= True)\n",
    "\n",
    "# region_count=[0]*66\n",
    "# for i in range(66):                 #-------storing each regions number of rows in the corresponding index, index 0 = region 1\n",
    "#     region_count[i] = final[final['start_region_id'] == i].shape[0]\n",
    "#     #region_count[i] = final['start_region_id'].value_counts()[i]\n",
    "\n",
    "# print(final)\n",
    "\n",
    "# i=0\n",
    "# for k in region_count:\n",
    "#     print(\"For region: \",i)\n",
    "#     print(\"number of rows: \",k)\n",
    "#     for j in range(k):\n",
    "#         print(\"indexing inside: \",j)\n",
    "#     i+=1\n",
    "\n",
    "# df_sorted.to_csv('mydata.csv', index=False)\n",
    "# print(df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------Extracting the total unique facilities provided over all from given poin\n",
    "\n",
    "# import pandas as pd\n",
    "# #-------Creating a set as it stores unique values\n",
    "# facilities = set()\n",
    "# #-------Looping over our poid data dictionary\n",
    "# for key, values in train_poi_data.items():\n",
    "#         #------Looping over the list of facilities for a specific region\n",
    "#     for value in values:\n",
    "#                 #-------splitting on semi colon to geth the facility type instead of the number of facility\n",
    "#         val=split_category(value)\n",
    "#         facilities.add(int(val))        #converting to int and storing so easier to sort\n",
    "\n",
    "# sorted_values = sorted(list(facilities))\n",
    "\n",
    "# for col in sorted_values:\n",
    "#         merged_df[col] = 0\n",
    "\n",
    "# #print(sorted_values)\n",
    "# #print(len(sorted_values))\n",
    "# #print(merged_df)\n",
    "\n",
    "# def check_facilities(vals, df, idx):\n",
    "#     for f in vals:\n",
    "#         value = split_category(f)\n",
    "#         if int(value) in df.columns:\n",
    "#             df.loc[idx, str(value)] = 1\n",
    "# # ---------for each region that has the facility replace the 0 with 1\n",
    "# # ----------------------to do, iterate over the data frame and for the regions of order put 1 \n",
    "# #for the facilities in that region(either start or destination)\n",
    "# for key, values in train_poi_data.items():\n",
    "#   # print(key,values)\n",
    "#   for index,row in merged_df.iterrows():\n",
    "#     if(key == row['start_region_hash']):\n",
    "#         check_facilities(values,merged_df,index)\n",
    "\n",
    "\n",
    "# print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if correcting merged and the hash values are correctly changed to cluster ID\n",
    "\n",
    "#---------example row\n",
    "# 9a864e958859b506f5f8bee9d8dfff17\torderID\n",
    "# a323121d71cd5247f38a4848c2039cb1\tdriverID\n",
    "# b9bd961ee676441d64c8748aa18efcda\tpassengerid\n",
    "# b05379ac3f9b7d99370d443cfd5dcc28\tstartregion\n",
    "# 52d7b69796362a8ed1691a6cc02ddde4   \tdestregion\n",
    "# 45.0\t\t\t\t\tprice\n",
    "# 2016-01-01 00:00:03\t\t\ttimestamp\n",
    "\n",
    "# result = train_cluster_map.loc[train_cluster_map['region_hash'] == '52d7b69796362a8ed1691a6cc02ddde4']\n",
    "# print(result) \n",
    "# result = final.loc[final['order_id'] == '9a864e958859b506f5f8bee9d8dfff17']\n",
    "# print(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create a sample dataframe with datetime column and values column\n",
    "df = pd.DataFrame({'datetime': pd.date_range('2022-01-01 00:00:00', '2022-01-01 01:00:00', freq='1min'), 'values': range(61)})\n",
    "# set the datetime column as index\n",
    "print(df)\n",
    "df = df.set_index('datetime')\n",
    "\n",
    "# resample to 10-minute intervals and fill missing values with interpolation\n",
    "df_10min = df.resample('10T').interpolate()\n",
    "\n",
    "# print the result\n",
    "print(df_10min)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blackconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
