{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "#loading test weather data\n",
    "# List all the weather data files\n",
    "file_pattern = 'test_set\\weather_data\\weather_data_*'\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# Read the files and store them in a list of DataFrames\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, sep='\\t', header=None, names=['timestamp', 'weather', 'temperature', 'pollution'])\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all the DataFrames into a single DataFrame\n",
    "test_weather_data=None\n",
    "if dataframes:\n",
    "    test_weather_data = pd.concat(dataframes)\n",
    "else:\n",
    "    exit('No data files found.')\n",
    "\n",
    "# Convert the 'timestamp' column to a datetime object\n",
    "test_weather_data['timestamp'] = pd.to_datetime(test_weather_data['timestamp'])\n",
    "\n",
    "# Reset the index and sort the DataFrame by the timestamp\n",
    "test_weather_data = test_weather_data.sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "print(test_weather_data.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the training weather data\n",
    "file_pattern = 'training_data\\\\weather_data\\\\weather_data_*'\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# Read the files and store them in a list of DataFrames\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, sep='\\t', header=None, names=['timestamp', 'weather', 'temperature', 'pollution'])\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all the DataFrames into a single DataFrame\n",
    "train_weather_data=None\n",
    "if dataframes:\n",
    "    train_weather_data = pd.concat(dataframes)\n",
    "else:\n",
    "    exit('No data files found.')\n",
    "\n",
    "# Convert the 'timestamp' column to a datetime object\n",
    "train_weather_data['timestamp'] = pd.to_datetime(train_weather_data['timestamp'])\n",
    "\n",
    "# Reset the index and sort the DataFrame by the timestamp\n",
    "train_weather_data = train_weather_data.sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "print(train_weather_data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "# loading the test order data\n",
    "# List all the order data files\n",
    "file_pattern = 'test_set/order_data/test_order_data_*'\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# Read the files and store them in a list of DataFrames\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, header=None, names=['order_id', 'driver_id', 'passenger_id', 'start_region_hash', 'Time'])\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all the DataFrames into a single DataFrame\n",
    "test_order_data = None\n",
    "if dataframes:\n",
    "    test_order_data = pd.concat(dataframes)\n",
    "else:\n",
    "    exit('No data files found.')\n",
    "\n",
    "# Convert the 'Time' column to a datetime object\n",
    "test_order_data['Time'] = pd.to_datetime(test_order_data['Time'])\n",
    "\n",
    "# Reset the index and sort the DataFrame by the timestamp\n",
    "test_order_data = test_order_data.sort_values(by='Time').reset_index(drop=True)\n",
    "\n",
    "print(test_order_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the training order data\n",
    "# read training data\n",
    "# List all the training data files\n",
    "file_pattern = 'training_data\\\\order_data\\\\order_data_*'\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# Read the files and store them in a list of DataFrames\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, header=None, sep='\\t', names=['order_id', 'driver_id', 'passenger_id', 'start_region_hash', 'dest_region_hash', 'Price', 'Time'])\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all the DataFrames into a single DataFrame\n",
    "training_order_data = None\n",
    "if dataframes:\n",
    "    training_order_data = pd.concat(dataframes)\n",
    "else:\n",
    "    exit('No data files found.')\n",
    "\n",
    "# Convert the 'timestamp' column to a datetime object\n",
    "training_order_data['Time'] = pd.to_datetime(training_order_data['Time'])\n",
    "\n",
    "# Reset the index and sort the DataFrame by the timestamp\n",
    "training_order_data = training_order_data.sort_values(by='Time').reset_index(drop=True)\n",
    "\n",
    "print(training_order_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test cluster map\n",
    "test_cluster_map = pd.read_csv('test_set\\\\cluster_map\\\\cluster_map', sep='\\t', header=None, names=['region_hash', 'cluster_id'])\n",
    "print(test_cluster_map.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training cluster map\n",
    "train_cluster_map = pd.read_csv('training_data\\\\cluster_map\\\\cluster_map', sep='\\t', header=None, names=['region_hash', 'cluster_id'])\n",
    "print(train_cluster_map.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load test poi data\n",
    "def read_file(filename):\n",
    "    region_data = {}\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            items = line.strip().split('\\t')\n",
    "            region_hash = items[0]\n",
    "            values = items[1:]\n",
    "            region_data[region_hash] = values\n",
    "    return region_data\n",
    "\n",
    "test_poi_data = read_file('test_set\\\\poi_data\\\\poi_data')\n",
    "print(test_poi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training poi data\n",
    "train_poi_data = read_file('training_data\\\\poi_data\\\\poi_data')\n",
    "print(train_poi_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  timestamp  weather  temperature  pollution  \\\n",
      "0       2016-01-01 00:00:28      1.0          4.0      177.0   \n",
      "1       2016-01-01 00:00:28      1.0          4.0      177.0   \n",
      "2       2016-01-01 00:00:28      1.0          4.0      177.0   \n",
      "3       2016-01-01 00:05:24      1.0          3.0      177.0   \n",
      "4       2016-01-01 00:05:24      1.0          3.0      177.0   \n",
      "...                     ...      ...          ...        ...   \n",
      "8541476                 NaT      NaN          NaN        NaN   \n",
      "8541477                 NaT      NaN          NaN        NaN   \n",
      "8541478                 NaT      NaN          NaN        NaN   \n",
      "8541479                 NaT      NaN          NaN        NaN   \n",
      "8541480                 NaT      NaN          NaN        NaN   \n",
      "\n",
      "                                 order_id                         driver_id  \\\n",
      "0        91d26804dadd84886ff1cdffaf9fbbfc  07b57eede4fb96c33a3e37635ca1e67e   \n",
      "1        7a24544eee9c840ae15519c280c4e1d2  0e0481eb0109c1a89a3efd134002b637   \n",
      "2        8bd1622f5d2086bfb7aea1ed18cb43f5  7dea8db04b91aa15048616afd94a27df   \n",
      "3        4a11fa9cca110ee0f76bfb9bc8293880  44349f15544d27ab8fb59f483494728c   \n",
      "4        1943eff83f1f6a4e58a15c6cd7e12dbf  e9e911eab98dda1860580133aa4756ee   \n",
      "...                                   ...                               ...   \n",
      "8541476  d91c420715aea6f2d20f77de04ebb6c1  b8c9f00445a7ee3e5b9c93920579323d   \n",
      "8541477  b4dfc35d7e15c53f109ae955345907b0                               NaN   \n",
      "8541478  6d79336e2c8b979de45b57f214991d76  bad7fbe113bb0272547b0df741e91718   \n",
      "8541479  9a7f01c745d48b81af3afae493357f2d  e068bd68e6df6f64c25f63d839c83bf4   \n",
      "8541480  3831a13720589ff58a3c047fa649c80f  3ca1d4ef5e4802ef869c85dc973dee67   \n",
      "\n",
      "                             passenger_id                 start_region_hash  \\\n",
      "0        583847ddf1a7ffc8994865dd32998bcf  ca064c2682ca48c6a21de012e87c0df5   \n",
      "1        82613e125946cfd371deb1adb61e1d9b  d4ec2125aff74eded207d2d915ef682f   \n",
      "2        07f0d35b3d26edae8ac8d4a3e77be635  3a43dcdff3c0b66b1acb1644ff055f9d   \n",
      "3        95f0c49e031f6d36efdb7b3f8b373a2b  91690261186ae5bee8f83808ea1e4a01   \n",
      "4        692a303c85b24aea392a1561992d805b  1cbfbdd079ef93e74405c53fcfff8567   \n",
      "...                                   ...                               ...   \n",
      "8541476  2cd6909b8057522d191ac6f49cf685f8  74c1c25f4b283fa74a5514307b0d0278   \n",
      "8541477  9f6eb7497789ecf60e490203a3209485  d4ec2125aff74eded207d2d915ef682f   \n",
      "8541478  86c0bab80c2bf0b329ed84d295d08467  74c1c25f4b283fa74a5514307b0d0278   \n",
      "8541479  55567011be80e492311be1ceaae5b1e1  4725c39a5e5f4c188d382da3910b3f3f   \n",
      "8541480  234f2901f37d77513e985890603df6c9  ca064c2682ca48c6a21de012e87c0df5   \n",
      "\n",
      "                         dest_region_hash  Price  \n",
      "0        d1ab2cc538d518758a1a82b1787592d4   54.0  \n",
      "1        b05379ac3f9b7d99370d443cfd5dcc28   34.0  \n",
      "2        3a43dcdff3c0b66b1acb1644ff055f9d    2.0  \n",
      "3        2407d482f0ffa22a947068f2551fe62c   20.0  \n",
      "4        1cbfbdd079ef93e74405c53fcfff8567   10.0  \n",
      "...                                   ...    ...  \n",
      "8541476  4b7f6f4e2bf237b6cc58f57142bea5c0   14.0  \n",
      "8541477  4725c39a5e5f4c188d382da3910b3f3f   23.0  \n",
      "8541478  74c1c25f4b283fa74a5514307b0d0278   10.0  \n",
      "8541479  82cc4851f9e4faa4e54309f8bb73fd7c   19.0  \n",
      "8541480  52a4e8aaa12f70020e889aed8fd5ddbc    9.0  \n",
      "\n",
      "[8541481 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "orderdummy= pd.read_csv('D:\\Python-Project\\AI\\dummyorder.txt', delimiter='\\t')\n",
    "weatherdummy= pd.read_csv('D:\\Python-Project\\AI\\dummyweather.txt', delimiter='\\t')\n",
    "\n",
    "\n",
    "#---------merging weather data and order data on time\n",
    "merged_df = pd.merge(train_weather_data, training_order_data, left_on='timestamp', right_on='Time',how='Left')\n",
    "merged_df = merged_df.drop(columns=['Time'])\n",
    "\n",
    "#-------------------Dummy data\n",
    "# merged_df = pd.merge(weatherdummy, orderdummy, left_on='timestamp', right_on='Time',how='outer')\n",
    "# merged_df = merged_df.drop(columns=['timestamp'])\n",
    "\n",
    "\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_category(value):\n",
    "  num = value.split(':')[0]\n",
    "                #-------Storing that value in our set\n",
    "        #print(num)\n",
    "  if('#' in num):\n",
    "      val=num.split('#')                  #-------getting the sub category only instead of main\n",
    "      val=(val[1])\n",
    "  else:\n",
    "      val=num\n",
    "  return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_facilities(vals,df,idx):\n",
    "  for f in vals:\n",
    "\n",
    "    value=split_category(f)\n",
    "    k=0\n",
    "    for col in df.columns:\n",
    "        \n",
    "    # if the column name matches the value\n",
    "        if(k > 10):\n",
    "          if int(col) == int(value):\n",
    "            # change the value of a specific row (e.g. row 0)\n",
    "            \n",
    "            df.loc[idx, col] = 1\n",
    "        k+=1\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  timestamp  weather  temperature  pollution  \\\n",
      "0       2016-01-01 00:00:28      1.0          4.0      177.0   \n",
      "1       2016-01-01 00:00:28      1.0          4.0      177.0   \n",
      "2       2016-01-01 00:00:28      1.0          4.0      177.0   \n",
      "3       2016-01-01 00:05:24      1.0          3.0      177.0   \n",
      "4       2016-01-01 00:05:24      1.0          3.0      177.0   \n",
      "...                     ...      ...          ...        ...   \n",
      "8541476                 NaT      NaN          NaN        NaN   \n",
      "8541477                 NaT      NaN          NaN        NaN   \n",
      "8541478                 NaT      NaN          NaN        NaN   \n",
      "8541479                 NaT      NaN          NaN        NaN   \n",
      "8541480                 NaT      NaN          NaN        NaN   \n",
      "\n",
      "                                 order_id                         driver_id  \\\n",
      "0        91d26804dadd84886ff1cdffaf9fbbfc  07b57eede4fb96c33a3e37635ca1e67e   \n",
      "1        7a24544eee9c840ae15519c280c4e1d2  0e0481eb0109c1a89a3efd134002b637   \n",
      "2        8bd1622f5d2086bfb7aea1ed18cb43f5  7dea8db04b91aa15048616afd94a27df   \n",
      "3        4a11fa9cca110ee0f76bfb9bc8293880  44349f15544d27ab8fb59f483494728c   \n",
      "4        1943eff83f1f6a4e58a15c6cd7e12dbf  e9e911eab98dda1860580133aa4756ee   \n",
      "...                                   ...                               ...   \n",
      "8541476  d91c420715aea6f2d20f77de04ebb6c1  b8c9f00445a7ee3e5b9c93920579323d   \n",
      "8541477  b4dfc35d7e15c53f109ae955345907b0                               NaN   \n",
      "8541478  6d79336e2c8b979de45b57f214991d76  bad7fbe113bb0272547b0df741e91718   \n",
      "8541479  9a7f01c745d48b81af3afae493357f2d  e068bd68e6df6f64c25f63d839c83bf4   \n",
      "8541480  3831a13720589ff58a3c047fa649c80f  3ca1d4ef5e4802ef869c85dc973dee67   \n",
      "\n",
      "                             passenger_id                 start_region_hash  \\\n",
      "0        583847ddf1a7ffc8994865dd32998bcf  ca064c2682ca48c6a21de012e87c0df5   \n",
      "1        82613e125946cfd371deb1adb61e1d9b  d4ec2125aff74eded207d2d915ef682f   \n",
      "2        07f0d35b3d26edae8ac8d4a3e77be635  3a43dcdff3c0b66b1acb1644ff055f9d   \n",
      "3        95f0c49e031f6d36efdb7b3f8b373a2b  91690261186ae5bee8f83808ea1e4a01   \n",
      "4        692a303c85b24aea392a1561992d805b  1cbfbdd079ef93e74405c53fcfff8567   \n",
      "...                                   ...                               ...   \n",
      "8541476  2cd6909b8057522d191ac6f49cf685f8  74c1c25f4b283fa74a5514307b0d0278   \n",
      "8541477  9f6eb7497789ecf60e490203a3209485  d4ec2125aff74eded207d2d915ef682f   \n",
      "8541478  86c0bab80c2bf0b329ed84d295d08467  74c1c25f4b283fa74a5514307b0d0278   \n",
      "8541479  55567011be80e492311be1ceaae5b1e1  4725c39a5e5f4c188d382da3910b3f3f   \n",
      "8541480  234f2901f37d77513e985890603df6c9  ca064c2682ca48c6a21de012e87c0df5   \n",
      "\n",
      "                         dest_region_hash  Price  ...  15  16  17  18  19  20  \\\n",
      "0        d1ab2cc538d518758a1a82b1787592d4   54.0  ...   0   0   0   0   0   0   \n",
      "1        b05379ac3f9b7d99370d443cfd5dcc28   34.0  ...   0   0   0   0   0   0   \n",
      "2        3a43dcdff3c0b66b1acb1644ff055f9d    2.0  ...   0   0   0   0   0   0   \n",
      "3        2407d482f0ffa22a947068f2551fe62c   20.0  ...   0   0   0   0   0   0   \n",
      "4        1cbfbdd079ef93e74405c53fcfff8567   10.0  ...   0   0   0   0   0   0   \n",
      "...                                   ...    ...  ...  ..  ..  ..  ..  ..  ..   \n",
      "8541476  4b7f6f4e2bf237b6cc58f57142bea5c0   14.0  ...   0   0   0   0   0   0   \n",
      "8541477  4725c39a5e5f4c188d382da3910b3f3f   23.0  ...   0   0   0   0   0   0   \n",
      "8541478  74c1c25f4b283fa74a5514307b0d0278   10.0  ...   0   0   0   0   0   0   \n",
      "8541479  82cc4851f9e4faa4e54309f8bb73fd7c   19.0  ...   0   0   0   0   0   0   \n",
      "8541480  52a4e8aaa12f70020e889aed8fd5ddbc    9.0  ...   0   0   0   0   0   0   \n",
      "\n",
      "         22  23  24  25  \n",
      "0         0   0   0   0  \n",
      "1         0   0   0   0  \n",
      "2         0   0   0   0  \n",
      "3         0   0   0   0  \n",
      "4         0   0   0   0  \n",
      "...      ..  ..  ..  ..  \n",
      "8541476   0   0   0   0  \n",
      "8541477   0   0   0   0  \n",
      "8541478   0   0   0   0  \n",
      "8541479   0   0   0   0  \n",
      "8541480   0   0   0   0  \n",
      "\n",
      "[8541481 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "#------Extracting the total unique facilities provided over all from given poin\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#-------Creating a set as it stores unique values\n",
    "facilities = set()\n",
    "\n",
    "#-------Looping over our poid data dictionary\n",
    "for key, values in train_poi_data.items():\n",
    "        #------Looping over the list of facilities for a specific region\n",
    "    for value in values:\n",
    "                #-------splitting on semi colon to geth the facility type instead of the number of facility\n",
    "        val=split_category(value)\n",
    "        facilities.add(int(val))        #converting to int and storing so easier to sort\n",
    "\n",
    "sorted_values = sorted(list(facilities))\n",
    "\n",
    "for col in sorted_values:\n",
    "        merged_df[col] = 0\n",
    "\n",
    "#print(sorted_values)\n",
    "# print(len(sorted_values))\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[148], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m   \u001b[39mfor\u001b[39;00m index,row \u001b[39min\u001b[39;00m merged_df\u001b[39m.\u001b[39miterrows():\n\u001b[0;32m      7\u001b[0m     \u001b[39mif\u001b[39;00m(key \u001b[39m==\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mstart_region_hash\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m----> 8\u001b[0m         check_facilities(values,merged_df,index)\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(merged_df)\n",
      "Cell \u001b[1;32mIn[146], line 13\u001b[0m, in \u001b[0;36mcheck_facilities\u001b[1;34m(vals, df, idx)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mif\u001b[39;00m(k \u001b[39m>\u001b[39m \u001b[39m10\u001b[39m):\n\u001b[0;32m     10\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mint\u001b[39m(col) \u001b[39m==\u001b[39m \u001b[39mint\u001b[39m(value):\n\u001b[0;32m     11\u001b[0m     \u001b[39m# change the value of a specific row (e.g. row 0)\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     df\u001b[39m.\u001b[39;49mloc[idx, col] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     14\u001b[0m k\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\fatima zubeda\\anaconda3\\envs\\pm3bap\\lib\\site-packages\\pandas\\core\\indexing.py:818\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    815\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    817\u001b[0m iloc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39miloc\n\u001b[1;32m--> 818\u001b[0m iloc\u001b[39m.\u001b[39;49m_setitem_with_indexer(indexer, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\fatima zubeda\\anaconda3\\envs\\pm3bap\\lib\\site-packages\\pandas\\core\\indexing.py:1795\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1792\u001b[0m \u001b[39m# align and set the values\u001b[39;00m\n\u001b[0;32m   1793\u001b[0m \u001b[39mif\u001b[39;00m take_split_path:\n\u001b[0;32m   1794\u001b[0m     \u001b[39m# We have to operate column-wise\u001b[39;00m\n\u001b[1;32m-> 1795\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setitem_with_indexer_split_path(indexer, value, name)\n\u001b[0;32m   1796\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1797\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[1;32mc:\\Users\\fatima zubeda\\anaconda3\\envs\\pm3bap\\lib\\site-packages\\pandas\\core\\indexing.py:1888\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_split_path\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1884\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1885\u001b[0m \n\u001b[0;32m   1886\u001b[0m     \u001b[39m# scalar value\u001b[39;00m\n\u001b[0;32m   1887\u001b[0m     \u001b[39mfor\u001b[39;00m loc \u001b[39min\u001b[39;00m ilocs:\n\u001b[1;32m-> 1888\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setitem_single_column(loc, value, pi)\n",
      "File \u001b[1;32mc:\\Users\\fatima zubeda\\anaconda3\\envs\\pm3bap\\lib\\site-packages\\pandas\\core\\indexing.py:1992\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_single_column\u001b[1;34m(self, loc, value, plane_indexer)\u001b[0m\n\u001b[0;32m   1988\u001b[0m         value \u001b[39m=\u001b[39m value[pi]\n\u001b[0;32m   1989\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1990\u001b[0m     \u001b[39m# set value into the column (first attempting to operate inplace, then\u001b[39;00m\n\u001b[0;32m   1991\u001b[0m     \u001b[39m#  falling back to casting if necessary)\u001b[39;00m\n\u001b[1;32m-> 1992\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mcolumn_setitem(loc, plane_indexer, value)\n\u001b[0;32m   1993\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_clear_item_cache()\n\u001b[0;32m   1994\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fatima zubeda\\anaconda3\\envs\\pm3bap\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1392\u001b[0m, in \u001b[0;36mBlockManager.column_setitem\u001b[1;34m(self, loc, idx, value, inplace)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1391\u001b[0m     new_mgr \u001b[39m=\u001b[39m col_mgr\u001b[39m.\u001b[39msetitem((idx,), value)\n\u001b[1;32m-> 1392\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miset(loc, new_mgr\u001b[39m.\u001b[39;49m_block\u001b[39m.\u001b[39;49mvalues, inplace\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\fatima zubeda\\anaconda3\\envs\\pm3bap\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1259\u001b[0m, in \u001b[0;36mBlockManager.iset\u001b[1;34m(self, loc, value, inplace)\u001b[0m\n\u001b[0;32m   1257\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clear_reference_block(blkno_l)\n\u001b[0;32m   1258\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1259\u001b[0m         blk\u001b[39m.\u001b[39;49mset_inplace(blk_locs, value_getitem(val_locs))\n\u001b[0;32m   1260\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1261\u001b[0m     unfit_mgr_locs\u001b[39m.\u001b[39mappend(blk\u001b[39m.\u001b[39mmgr_locs\u001b[39m.\u001b[39mas_array[blk_locs])\n",
      "File \u001b[1;32mc:\\Users\\fatima zubeda\\anaconda3\\envs\\pm3bap\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:859\u001b[0m, in \u001b[0;36mBlock.set_inplace\u001b[1;34m(self, locs, values, copy)\u001b[0m\n\u001b[0;32m    857\u001b[0m \u001b[39mif\u001b[39;00m copy:\n\u001b[0;32m    858\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m--> 859\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalues[locs] \u001b[39m=\u001b[39m values\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#---------for each regiong that has the facility replace the 0 with 1\n",
    "#----------------------to do, iterate over the data frame and for the regions of order put 1 \n",
    "#for the facilities in that region(either start or destination)\n",
    "for key, values in train_poi_data.items():\n",
    "  # print(key,values)\n",
    "  for index,row in merged_df.iterrows():\n",
    "    if(key == row['start_region_hash']):\n",
    "        check_facilities(values,merged_df,index)\n",
    "        \n",
    "\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(merged_df)\n",
    "\n",
    "#---------merging the merged data with cluster map to replace the region hash value with its cluster ID value\n",
    "final = pd.merge(merged_df, train_cluster_map, left_on='start_region_hash', right_on='region_hash')\n",
    "final = final.drop(columns=['region_hash','start_region_hash'])\n",
    "final = final.rename(columns={'cluster_id': 'start_region_id'})\n",
    "\n",
    "final = pd.merge(final, train_cluster_map, left_on='dest_region_hash', right_on='region_hash')\n",
    "final = final.drop(columns=['region_hash','dest_region_hash'])\n",
    "final = final.rename(columns={'cluster_id': 'dest_region_id'})\n",
    "\n",
    "\n",
    "#---------extracting day of the week from the given time stamp\n",
    "final['Time']=pd.to_datetime(final['Time'])\n",
    "\n",
    "final['day']=(final['Time'].dt.day_name())\n",
    "\n",
    "# print(\"column count: \",final.shape[1])\n",
    "# print(\"row count\",final.count())\n",
    "print(final.head())\n",
    "\n",
    "#-------------Sorting the table first according to region then within those regions, sorting according to time \n",
    "sorted_final = final.sort_values(by=['start_region_id', 'Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------Finding the Demand-supply gap for each ten minutes\n",
    "time_slot= final.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    weather  temperature  PM2.5                          order_id  \\\n",
      "72      NaN          NaN    NaN  9992050d211a1cfd10ed3ce538c43d11   \n",
      "73      NaN          NaN    NaN  d04015c1df1059e7686162b835881768   \n",
      "69      NaN          NaN    NaN  a450d7b889f62a911263bac6b88fb0bf   \n",
      "70      NaN          NaN    NaN  28f619724fad341f07020f7ebeee43ab   \n",
      "67      NaN          NaN    NaN  49b946538b8f03be66a0e1a56cd181e9   \n",
      "..      ...          ...    ...                               ...   \n",
      "14      NaN          NaN    NaN  5d90c0268db61573fa43413764bb688c   \n",
      "22      NaN          NaN    NaN  d625ca9967f88687dbc9fdf31a16fa15   \n",
      "23      NaN          NaN    NaN  ab6d984505b1cc0e3e49e32c8b714432   \n",
      "76      NaN          NaN    NaN  388ef65288d40ed959bbb2977d507627   \n",
      "68      NaN          NaN    NaN  674334bb413856fb9ccbcff7e4e2c7a6   \n",
      "\n",
      "                           driver_id                      passenger_id  Price  \\\n",
      "72  c5060c97b34453fb71ff6c7839a3ca56  fa090bdff73c40aa3d8e4a48667bad83    6.0   \n",
      "73  56e342a92a8f949e9c7730cf67058c19  6ce4962c8dd74aa00115438dab66a898    8.0   \n",
      "69                               NaN  e97bf99e67f41bdab02b187fce39581c   10.0   \n",
      "70  4abbb8b77dc1e6d543a511b32c612626  e97bf99e67f41bdab02b187fce39581c   10.0   \n",
      "67  fecef8b293d14afb05d4ae95bbbd5daa  6522c7d747fa344fab58959b30e8cf21    3.0   \n",
      "..                               ...                               ...    ...   \n",
      "14  4f4b965da14b5313be6741d9248ffd6b  4a77c2f82586ea9624b9f929603b6d20   13.7   \n",
      "22  8599e52d9313e909d778b140d1a672f4  be0aa0454e61c90edcd81e50d3fc86f1   13.7   \n",
      "23  8d48ffc5bb7ef189ed3d45106aa6ee09  f9a5eece6d6c36bb6f97d0f5bebc4181   13.4   \n",
      "76  e205bb6eb022b0dfaa92c293e89267b6  677db12277ac14d6f7f46d19132cd59d    7.0   \n",
      "68  30b21c664c364d8172ca67cf67f1abcd  5cffa2cf97c2d17a84486a5185e957d7    5.0   \n",
      "\n",
      "                  Time  1  2  ...  18  19  20  22  23  24  25  \\\n",
      "72 2016-01-05 07:13:21  0  1  ...   1   1   1   1   1   1   1   \n",
      "73 2016-01-05 18:56:28  0  1  ...   1   1   1   1   1   1   1   \n",
      "69 2016-01-05 16:07:02  0  1  ...   1   1   1   1   1   1   1   \n",
      "70 2016-01-05 16:07:02  0  1  ...   1   1   1   1   1   1   1   \n",
      "67 2016-01-05 17:22:42  0  1  ...   1   1   1   1   1   1   1   \n",
      "..                 ... .. ..  ...  ..  ..  ..  ..  ..  ..  ..   \n",
      "14 2016-01-05 17:28:11  0  1  ...   1   1   1   1   1   1   1   \n",
      "22 2016-01-05 17:44:39  0  1  ...   1   1   1   1   1   1   1   \n",
      "23 2016-01-05 21:27:53  0  1  ...   1   1   1   1   1   1   1   \n",
      "76 2016-01-05 13:25:15  0  1  ...   0   1   1   1   0   1   0   \n",
      "68 2016-01-05 08:44:03  0  1  ...   1   1   1   1   1   1   1   \n",
      "\n",
      "    start_region_id  dest_region_id      day  \n",
      "72                1               1  Tuesday  \n",
      "73                1               1  Tuesday  \n",
      "69                2              57  Tuesday  \n",
      "70                2              57  Tuesday  \n",
      "67                2               2  Tuesday  \n",
      "..              ...             ...      ...  \n",
      "14               51               7  Tuesday  \n",
      "22               51              48  Tuesday  \n",
      "23               51              48  Tuesday  \n",
      "76               56              56  Tuesday  \n",
      "68               57               2  Tuesday  \n",
      "\n",
      "[79 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------ignore\n",
    "\n",
    "# df_sorted = final.sort_values(by=['start_region_id'], ascending= True)\n",
    "\n",
    "# region_count=[0]*66\n",
    "# for i in range(66):                 #-------storing each regions number of rows in the corresponding index, index 0 = region 1\n",
    "#     region_count[i] = final[final['start_region_id'] == i].shape[0]\n",
    "#     #region_count[i] = final['start_region_id'].value_counts()[i]\n",
    "\n",
    "# print(final)\n",
    "\n",
    "# i=0\n",
    "# for k in region_count:\n",
    "#     print(\"For region: \",i)\n",
    "#     print(\"number of rows: \",k)\n",
    "#     for j in range(k):\n",
    "#         print(\"indexing inside: \",j)\n",
    "#     i+=1\n",
    "\n",
    "# df_sorted.to_csv('mydata.csv', index=False)\n",
    "# print(df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if correcting merged and the hash values are correctly changed to cluster ID\n",
    "\n",
    "#---------example row\n",
    "# 9a864e958859b506f5f8bee9d8dfff17\torderID\n",
    "# a323121d71cd5247f38a4848c2039cb1\tdriverID\n",
    "# b9bd961ee676441d64c8748aa18efcda\tpassengerid\n",
    "# b05379ac3f9b7d99370d443cfd5dcc28\tstartregion\n",
    "# 52d7b69796362a8ed1691a6cc02ddde4   \tdestregion\n",
    "# 45.0\t\t\t\t\tprice\n",
    "# 2016-01-01 00:00:03\t\t\ttimestamp\n",
    "\n",
    "result = train_cluster_map.loc[train_cluster_map['region_hash'] == '52d7b69796362a8ed1691a6cc02ddde4']\n",
    "print(result) \n",
    "result = final.loc[final['order_id'] == '9a864e958859b506f5f8bee9d8dfff17']\n",
    "print(result)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blackconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
