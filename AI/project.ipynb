{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "#loading test weather data\n",
    "# List all the weather data files\n",
    "file_pattern = 'test_set\\weather_data\\weather_data_*'\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# Read the files and store them in a list of DataFrames\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, sep='\\t', header=None, names=['timestamp', 'weather', 'temperature', 'pollution'])\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all the DataFrames into a single DataFrame\n",
    "test_weather_data=None\n",
    "if dataframes:\n",
    "    test_weather_data = pd.concat(dataframes)\n",
    "else:\n",
    "    exit('No data files found.')\n",
    "\n",
    "# Convert the 'timestamp' column to a datetime object\n",
    "test_weather_data['timestamp'] = pd.to_datetime(test_weather_data['timestamp'])\n",
    "\n",
    "# Reset the index and sort the DataFrame by the timestamp\n",
    "test_weather_data = test_weather_data.sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "print(test_weather_data.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the training weather data\n",
    "file_pattern = 'training_data\\\\weather_data\\\\weather_data_*'\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# Read the files and store them in a list of DataFrames\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, sep='\\t', header=None, names=['timestamp', 'weather', 'temperature', 'pollution'])\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all the DataFrames into a single DataFrame\n",
    "train_weather_data=None\n",
    "if dataframes:\n",
    "    train_weather_data = pd.concat(dataframes)\n",
    "else:\n",
    "    exit('No data files found.')\n",
    "\n",
    "# Convert the 'timestamp' column to a datetime object\n",
    "train_weather_data['timestamp'] = pd.to_datetime(train_weather_data['timestamp'])\n",
    "\n",
    "# Reset the index and sort the DataFrame by the timestamp\n",
    "train_weather_data = train_weather_data.sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "print(train_weather_data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "# loading the test order data\n",
    "# List all the order data files\n",
    "file_pattern = 'test_set/order_data/test_order_data_*'\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# Read the files and store them in a list of DataFrames\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, header=None, names=['order_id', 'driver_id', 'passenger_id', 'start_region_hash', 'Time'])\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all the DataFrames into a single DataFrame\n",
    "test_order_data = None\n",
    "if dataframes:\n",
    "    test_order_data = pd.concat(dataframes)\n",
    "else:\n",
    "    exit('No data files found.')\n",
    "\n",
    "# Convert the 'Time' column to a datetime object\n",
    "test_order_data['Time'] = pd.to_datetime(test_order_data['Time'])\n",
    "\n",
    "# Reset the index and sort the DataFrame by the timestamp\n",
    "test_order_data = test_order_data.sort_values(by='Time').reset_index(drop=True)\n",
    "\n",
    "print(test_order_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the training order data\n",
    "# read training data\n",
    "# List all the training data files\n",
    "file_pattern = 'training_data\\\\order_data\\\\order_data_*'\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# Read the files and store them in a list of DataFrames\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, header=None, sep='\\t', names=['order_id', 'driver_id', 'passenger_id', 'start_region_hash', 'dest_region_hash', 'Price', 'Time'])\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all the DataFrames into a single DataFrame\n",
    "training_order_data = None\n",
    "if dataframes:\n",
    "    training_order_data = pd.concat(dataframes)\n",
    "else:\n",
    "    exit('No data files found.')\n",
    "\n",
    "# Convert the 'timestamp' column to a datetime object\n",
    "training_order_data['Time'] = pd.to_datetime(training_order_data['Time'])\n",
    "\n",
    "# Reset the index and sort the DataFrame by the timestamp\n",
    "training_order_data = training_order_data.sort_values(by='Time').reset_index(drop=True)\n",
    "\n",
    "print(training_order_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test cluster map\n",
    "test_cluster_map = pd.read_csv('test_set\\\\cluster_map\\\\cluster_map', sep='\\t', header=None, names=['region_hash', 'cluster_id'])\n",
    "print(test_cluster_map.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training cluster map\n",
    "train_cluster_map = pd.read_csv('training_data\\\\cluster_map\\\\cluster_map', sep='\\t', header=None, names=['region_hash', 'cluster_id'])\n",
    "print(train_cluster_map.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load test poi data\n",
    "def read_file(filename):\n",
    "    region_data = {}\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            items = line.strip().split('\\t')\n",
    "            region_hash = items[0]\n",
    "            values = items[1:]\n",
    "            region_data[region_hash] = values\n",
    "    return region_data\n",
    "\n",
    "test_poi_data = read_file('test_set\\\\poi_data\\\\poi_data')\n",
    "print(test_poi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training poi data\n",
    "train_poi_data = read_file('training_data\\\\poi_data\\\\poi_data')\n",
    "print(train_poi_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# orderdummy= pd.read_csv('D:\\Python-Project\\AI\\dummyorder.txt', delimiter='\\t')\n",
    "# weatherdummy= pd.read_csv('D:\\Python-Project\\AI\\dummyweather.txt', delimiter='\\t')\n",
    "\n",
    "\n",
    "#---------merging weather data and order data on time\n",
    "merged_df = pd.merge(train_weather_data, training_order_data, left_on='timestamp', right_on='Time',how='right')\n",
    "merged_df = merged_df.drop(columns=['timestamp'])\n",
    "\n",
    "# -------------------Dummy data\n",
    "# merged_df = pd.merge(weatherdummy, orderdummy, left_on='timestamp', right_on='Time',how='outer')\n",
    "# merged_df = merged_df.drop(columns=['timestamp'])\n",
    "\n",
    "\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_category(value):\n",
    "  num = value.split(':')[0]\n",
    "                #-------Storing that value in our set\n",
    "        #print(num)\n",
    "  if('#' in num):\n",
    "      val=num.split('#')                  #-------getting the sub category only instead of main\n",
    "      val=(val[1])\n",
    "  else:\n",
    "      val=num\n",
    "  return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_facilities(vals, df, idx):\n",
    "#     for f in vals:\n",
    "#         value = split_category(f)\n",
    "#         # col_num = int(value)\n",
    "#         # if col_num in df.columns[:11]:\n",
    "#         #     df.iat[idx, col_num] = 1\n",
    "#         if int(value) in df.columns:\n",
    "#             df.loc[idx, str(value)] = 1\n",
    "def check_facilities(row, train_poi_data):\n",
    "    facilities = train_poi_data.get(row['start_region_hash'], [])\n",
    "\n",
    "    for facility in facilities:\n",
    "        value = split_category(facility)\n",
    "        if str(value) in row.index:\n",
    "            row[str(value)] = 1\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------Extracting the total unique facilities provided over all from given poin\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# #-------Creating a set as it stores unique values\n",
    "facilities = set()\n",
    "\n",
    "# #-------Looping over our poid data dictionary\n",
    "for key, values in train_poi_data.items():\n",
    "        #------Looping over the list of facilities for a specific region\n",
    "    for value in values:\n",
    "                #-------splitting on semi colon to geth the facility type instead of the number of facility\n",
    "        val=split_category(value)\n",
    "        facilities.add(int(val))        #converting to int and storing so easier to sort\n",
    "\n",
    "sorted_values = sorted(list(facilities))\n",
    "\n",
    "for col in sorted_values:\n",
    "        merged_df[col] = 0\n",
    "\n",
    "# #print(sorted_values)\n",
    "# # print(len(sorted_values))\n",
    "# print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming train_poi_data is a dictionary with start_region_hash as key and a list of facilities as value\n",
    "# Assuming merged_df is a DataFrame with 'start_region_hash' column\n",
    "\n",
    "# Get all unique facilities from the train_poi_data dictionary\n",
    "# unique_facilities = set()\n",
    "# for facilities in train_poi_data.values():\n",
    "#     for facility in facilities:\n",
    "#         unique_facilities.add(split_category(facility))\n",
    "\n",
    "# # Ensure that all unique facilities have columns in merged_df, initialized with 0\n",
    "# for facility in unique_facilities:\n",
    "#     if str(facility) not in merged_df.columns:\n",
    "#         merged_df[str(facility)] = 0\n",
    "\n",
    "# Update the merged_df using the check_facilities function\n",
    "merged_df = merged_df.apply(lambda row: check_facilities(row, train_poi_data), axis=1)\n",
    "\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, values in train_poi_data.items():\n",
    "  # print(key,values)\n",
    "  for index,row in merged_df.iterrows():\n",
    "    if(key == row['start_region_hash']):\n",
    "        check_facilities(values,merged_df,index)\n",
    "# # merged_df.to_csv('mydata.csv', index=False)\n",
    "# print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#---------merging the merged data with cluster map to replace the region hash value with its cluster ID value\n",
    "final = pd.merge(merged_df, train_cluster_map, left_on='start_region_hash', right_on='region_hash')\n",
    "final = final.drop(columns=['region_hash','start_region_hash'])\n",
    "final = final.rename(columns={'cluster_id': 'start_region_id'})\n",
    "\n",
    "final = pd.merge(final, train_cluster_map, left_on='dest_region_hash', right_on='region_hash')\n",
    "final = final.drop(columns=['region_hash','dest_region_hash'])\n",
    "final = final.rename(columns={'cluster_id': 'dest_region_id'})\n",
    "\n",
    "\n",
    "#---------extracting day of the week from the given time stamp\n",
    "final['Time']=pd.to_datetime(final['Time'])\n",
    "final['day']=(final['Time'].dt.day_name())\n",
    "\n",
    "# print(\"column count: \",final.shape[1])\n",
    "# print(\"row count\",final.count())\n",
    "#print(final.head())\n",
    "\n",
    "#-------------Sorting the table first according to region then within those regions, sorting according to time \n",
    "sorted_final = final.sort_values(by=['start_region_id', 'Time'])\n",
    "\n",
    "# print(sorted_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------Finding the Demand-supply gap for each ten minutes\n",
    "time_slot= final.copy(deep=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------ignore\n",
    "\n",
    "# df_sorted = final.sort_values(by=['start_region_id'], ascending= True)\n",
    "\n",
    "# region_count=[0]*66\n",
    "# for i in range(66):                 #-------storing each regions number of rows in the corresponding index, index 0 = region 1\n",
    "#     region_count[i] = final[final['start_region_id'] == i].shape[0]\n",
    "#     #region_count[i] = final['start_region_id'].value_counts()[i]\n",
    "\n",
    "# print(final)\n",
    "\n",
    "# i=0\n",
    "# for k in region_count:\n",
    "#     print(\"For region: \",i)\n",
    "#     print(\"number of rows: \",k)\n",
    "#     for j in range(k):\n",
    "#         print(\"indexing inside: \",j)\n",
    "#     i+=1\n",
    "\n",
    "# df_sorted.to_csv('mydata.csv', index=False)\n",
    "# print(df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if correcting merged and the hash values are correctly changed to cluster ID\n",
    "\n",
    "#---------example row\n",
    "# 9a864e958859b506f5f8bee9d8dfff17\torderID\n",
    "# a323121d71cd5247f38a4848c2039cb1\tdriverID\n",
    "# b9bd961ee676441d64c8748aa18efcda\tpassengerid\n",
    "# b05379ac3f9b7d99370d443cfd5dcc28\tstartregion\n",
    "# 52d7b69796362a8ed1691a6cc02ddde4   \tdestregion\n",
    "# 45.0\t\t\t\t\tprice\n",
    "# 2016-01-01 00:00:03\t\t\ttimestamp\n",
    "\n",
    "result = train_cluster_map.loc[train_cluster_map['region_hash'] == '52d7b69796362a8ed1691a6cc02ddde4']\n",
    "print(result) \n",
    "result = final.loc[final['order_id'] == '9a864e958859b506f5f8bee9d8dfff17']\n",
    "print(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create a sample dataframe with datetime column and values column\n",
    "df = pd.DataFrame({'datetime': pd.date_range('2022-01-01 00:00:00', '2022-01-01 01:00:00', freq='1min'), 'values': range(61)})\n",
    "# set the datetime column as index\n",
    "print(df)\n",
    "df = df.set_index('datetime')\n",
    "\n",
    "# resample to 10-minute intervals and fill missing values with interpolation\n",
    "df_10min = df.resample('10T').interpolate()\n",
    "\n",
    "# print the result\n",
    "print(df_10min)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blackconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
