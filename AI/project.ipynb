{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "#loading test weather data\n",
    "# List all the weather data files\n",
    "file_pattern = 'test_set\\weather_data\\weather_data_*'\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# Read the files and store them in a list of DataFrames\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, sep='\\t', header=None, names=['timestamp', 'weather', 'temperature', 'pollution'])\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all the DataFrames into a single DataFrame\n",
    "test_weather_data=None\n",
    "if dataframes:\n",
    "    test_weather_data = pd.concat(dataframes)\n",
    "else:\n",
    "    exit('No data files found.')\n",
    "\n",
    "# Convert the 'timestamp' column to a datetime object\n",
    "test_weather_data['timestamp'] = pd.to_datetime(test_weather_data['timestamp'])\n",
    "\n",
    "# Reset the index and sort the DataFrame by the timestamp\n",
    "test_weather_data = test_weather_data.sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "print(test_weather_data.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the training weather data\n",
    "file_pattern = 'training_data\\\\weather_data\\\\weather_data_*'\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# Read the files and store them in a list of DataFrames\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, sep='\\t', header=None, names=['timestamp', 'weather', 'temperature', 'pollution'])\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all the DataFrames into a single DataFrame\n",
    "train_weather_data=None\n",
    "if dataframes:\n",
    "    train_weather_data = pd.concat(dataframes)\n",
    "else:\n",
    "    exit('No data files found.')\n",
    "\n",
    "# Convert the 'timestamp' column to a datetime object\n",
    "train_weather_data['timestamp'] = pd.to_datetime(train_weather_data['timestamp'])\n",
    "\n",
    "# Reset the index and sort the DataFrame by the timestamp\n",
    "train_weather_data = train_weather_data.sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "print(train_weather_data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "# loading the test order data\n",
    "# List all the order data files\n",
    "file_pattern = 'test_set/order_data/test_order_data_*'\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# Read the files and store them in a list of DataFrames\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, header=None, names=['order_id', 'driver_id', 'passenger_id', 'start_region_hash', 'Time'])\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all the DataFrames into a single DataFrame\n",
    "test_order_data = None\n",
    "if dataframes:\n",
    "    test_order_data = pd.concat(dataframes)\n",
    "else:\n",
    "    exit('No data files found.')\n",
    "\n",
    "# Convert the 'Time' column to a datetime object\n",
    "test_order_data['Time'] = pd.to_datetime(test_order_data['Time'])\n",
    "\n",
    "# Reset the index and sort the DataFrame by the timestamp\n",
    "test_order_data = test_order_data.sort_values(by='Time').reset_index(drop=True)\n",
    "\n",
    "print(test_order_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the training order data\n",
    "# read training data\n",
    "# List all the training data files\n",
    "file_pattern = 'training_data\\\\order_data\\\\order_data_*'\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# Read the files and store them in a list of DataFrames\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, header=None, sep='\\t', names=['order_id', 'driver_id', 'passenger_id', 'start_region_hash', 'dest_region_hash', 'Price', 'Time'])\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all the DataFrames into a single DataFrame\n",
    "training_order_data = None\n",
    "if dataframes:\n",
    "    training_order_data = pd.concat(dataframes)\n",
    "else:\n",
    "    exit('No data files found.')\n",
    "\n",
    "# Convert the 'timestamp' column to a datetime object\n",
    "training_order_data['Time'] = pd.to_datetime(training_order_data['Time'])\n",
    "\n",
    "# Reset the index and sort the DataFrame by the timestamp\n",
    "training_order_data = training_order_data.sort_values(by='Time').reset_index(drop=True)\n",
    "\n",
    "print(training_order_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test cluster map\n",
    "test_cluster_map = pd.read_csv('test_set\\\\cluster_map\\\\cluster_map', sep='\\t', header=None, names=['region_hash', 'cluster_id'])\n",
    "print(test_cluster_map.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training cluster map\n",
    "train_cluster_map = pd.read_csv('training_data\\\\cluster_map\\\\cluster_map', sep='\\t', header=None, names=['region_hash', 'cluster_id'])\n",
    "print(train_cluster_map.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load test poi data\n",
    "def read_file(filename):\n",
    "    region_data = {}\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            items = line.strip().split('\\t')\n",
    "            region_hash = items[0]\n",
    "            values = items[1:]\n",
    "            region_data[region_hash] = values\n",
    "    return region_data\n",
    "\n",
    "test_poi_data = read_file('test_set\\\\poi_data\\\\poi_data')\n",
    "print(test_poi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training poi data\n",
    "train_poi_data = read_file('training_data\\\\poi_data\\\\poi_data')\n",
    "print(train_poi_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "orderdummy= pd.read_csv('D:\\Python-Project\\AI\\dummyorder.txt', delimiter='\\t')\n",
    "weatherdummy= pd.read_csv('D:\\Python-Project\\AI\\dummyweather.txt', delimiter='\\t')\n",
    "\n",
    "\n",
    "#---------merging weather data and order data on time\n",
    "# merged_df = pd.merge(train_weather_data, training_order_data, left_on='timestamp', right_on='Time',how='outer')\n",
    "# merged_df = merged_df.drop(columns=['Time'])\n",
    "\n",
    "#-------------------Dummy data\n",
    "merged_df = pd.merge(weatherdummy, orderdummy, left_on='timestamp', right_on='Time',how='outer')\n",
    "merged_df = merged_df.drop(columns=['timestamp'])\n",
    "\n",
    "\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_category(value):\n",
    "  num = value.split(':')[0]\n",
    "                #-------Storing that value in our set\n",
    "        #print(num)\n",
    "  if('#' in num):\n",
    "      val=num.split('#')                  #-------getting the sub category only instead of main\n",
    "      val=(val[1])\n",
    "  else:\n",
    "      val=num\n",
    "  return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_facilities(vals,df,idx):\n",
    "  for f in vals:\n",
    "\n",
    "    value=split_category(f)\n",
    "    k=0\n",
    "    for col in df.columns:\n",
    "        \n",
    "    # if the column name matches the value\n",
    "        if(k > 10):\n",
    "          if int(col) == int(value):\n",
    "            # change the value of a specific row (e.g. row 0)\n",
    "            \n",
    "            df.loc[idx, col] = 1\n",
    "        k+=1\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------Extracting the total unique facilities provided over all from given poin\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#-------Creating a set as it stores unique values\n",
    "facilities = set()\n",
    "\n",
    "#-------Looping over our poid data dictionary\n",
    "for key, values in train_poi_data.items():\n",
    "        #------Looping over the list of facilities for a specific region\n",
    "    for value in values:\n",
    "                #-------splitting on semi colon to geth the facility type instead of the number of facility\n",
    "        val=split_category(value)\n",
    "        facilities.add(int(val))        #converting to int and storing so easier to sort\n",
    "\n",
    "sorted_values = sorted(list(facilities))\n",
    "\n",
    "for col in sorted_values:\n",
    "        merged_df[col] = 0\n",
    "\n",
    "#print(sorted_values)\n",
    "# print(len(sorted_values))\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------for each regiong that has the facility replace the 0 with 1\n",
    "#----------------------to do, iterate over the data frame and for the regions of order put 1 \n",
    "#for the facilities in that region(either start or destination)\n",
    "for key, values in train_poi_data.items():\n",
    "  # print(key,values)\n",
    "  for index,row in merged_df.iterrows():\n",
    "    if(key == row['start_region_hash']):\n",
    "        check_facilities(values,merged_df,index)\n",
    "        \n",
    "\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(merged_df)\n",
    "\n",
    "#---------merging the merged data with cluster map to replace the region hash value with its cluster ID value\n",
    "final = pd.merge(merged_df, train_cluster_map, left_on='start_region_hash', right_on='region_hash')\n",
    "final = final.drop(columns=['region_hash','start_region_hash'])\n",
    "final = final.rename(columns={'cluster_id': 'start_region_id'})\n",
    "\n",
    "final = pd.merge(final, train_cluster_map, left_on='dest_region_hash', right_on='region_hash')\n",
    "final = final.drop(columns=['region_hash','dest_region_hash'])\n",
    "final = final.rename(columns={'cluster_id': 'dest_region_id'})\n",
    "\n",
    "\n",
    "#---------extracting day of the week from the given time stamp\n",
    "final['Time']=pd.to_datetime(final['Time'])\n",
    "\n",
    "final['day']=(final['Time'].dt.day_name())\n",
    "\n",
    "# print(\"column count: \",final.shape[1])\n",
    "# print(\"row count\",final.count())\n",
    "print(final.head())\n",
    "\n",
    "#-------------Sorting the table first according to region then within those regions, sorting according to time \n",
    "sorted_final = final.sort_values(by=['start_region_id', 'Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    weather  temperature  PM2.5                          order_id  \\\n",
      "72      NaN          NaN    NaN  9992050d211a1cfd10ed3ce538c43d11   \n",
      "73      NaN          NaN    NaN  d04015c1df1059e7686162b835881768   \n",
      "69      NaN          NaN    NaN  a450d7b889f62a911263bac6b88fb0bf   \n",
      "70      NaN          NaN    NaN  28f619724fad341f07020f7ebeee43ab   \n",
      "67      NaN          NaN    NaN  49b946538b8f03be66a0e1a56cd181e9   \n",
      "..      ...          ...    ...                               ...   \n",
      "14      NaN          NaN    NaN  5d90c0268db61573fa43413764bb688c   \n",
      "22      NaN          NaN    NaN  d625ca9967f88687dbc9fdf31a16fa15   \n",
      "23      NaN          NaN    NaN  ab6d984505b1cc0e3e49e32c8b714432   \n",
      "76      NaN          NaN    NaN  388ef65288d40ed959bbb2977d507627   \n",
      "68      NaN          NaN    NaN  674334bb413856fb9ccbcff7e4e2c7a6   \n",
      "\n",
      "                           driver_id                      passenger_id  Price  \\\n",
      "72  c5060c97b34453fb71ff6c7839a3ca56  fa090bdff73c40aa3d8e4a48667bad83    6.0   \n",
      "73  56e342a92a8f949e9c7730cf67058c19  6ce4962c8dd74aa00115438dab66a898    8.0   \n",
      "69                               NaN  e97bf99e67f41bdab02b187fce39581c   10.0   \n",
      "70  4abbb8b77dc1e6d543a511b32c612626  e97bf99e67f41bdab02b187fce39581c   10.0   \n",
      "67  fecef8b293d14afb05d4ae95bbbd5daa  6522c7d747fa344fab58959b30e8cf21    3.0   \n",
      "..                               ...                               ...    ...   \n",
      "14  4f4b965da14b5313be6741d9248ffd6b  4a77c2f82586ea9624b9f929603b6d20   13.7   \n",
      "22  8599e52d9313e909d778b140d1a672f4  be0aa0454e61c90edcd81e50d3fc86f1   13.7   \n",
      "23  8d48ffc5bb7ef189ed3d45106aa6ee09  f9a5eece6d6c36bb6f97d0f5bebc4181   13.4   \n",
      "76  e205bb6eb022b0dfaa92c293e89267b6  677db12277ac14d6f7f46d19132cd59d    7.0   \n",
      "68  30b21c664c364d8172ca67cf67f1abcd  5cffa2cf97c2d17a84486a5185e957d7    5.0   \n",
      "\n",
      "                  Time  1  2  ...  18  19  20  22  23  24  25  \\\n",
      "72 2016-01-05 07:13:21  0  1  ...   1   1   1   1   1   1   1   \n",
      "73 2016-01-05 18:56:28  0  1  ...   1   1   1   1   1   1   1   \n",
      "69 2016-01-05 16:07:02  0  1  ...   1   1   1   1   1   1   1   \n",
      "70 2016-01-05 16:07:02  0  1  ...   1   1   1   1   1   1   1   \n",
      "67 2016-01-05 17:22:42  0  1  ...   1   1   1   1   1   1   1   \n",
      "..                 ... .. ..  ...  ..  ..  ..  ..  ..  ..  ..   \n",
      "14 2016-01-05 17:28:11  0  1  ...   1   1   1   1   1   1   1   \n",
      "22 2016-01-05 17:44:39  0  1  ...   1   1   1   1   1   1   1   \n",
      "23 2016-01-05 21:27:53  0  1  ...   1   1   1   1   1   1   1   \n",
      "76 2016-01-05 13:25:15  0  1  ...   0   1   1   1   0   1   0   \n",
      "68 2016-01-05 08:44:03  0  1  ...   1   1   1   1   1   1   1   \n",
      "\n",
      "    start_region_id  dest_region_id      day  \n",
      "72                1               1  Tuesday  \n",
      "73                1               1  Tuesday  \n",
      "69                2              57  Tuesday  \n",
      "70                2              57  Tuesday  \n",
      "67                2               2  Tuesday  \n",
      "..              ...             ...      ...  \n",
      "14               51               7  Tuesday  \n",
      "22               51              48  Tuesday  \n",
      "23               51              48  Tuesday  \n",
      "76               56              56  Tuesday  \n",
      "68               57               2  Tuesday  \n",
      "\n",
      "[79 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------ignore\n",
    "\n",
    "# df_sorted = final.sort_values(by=['start_region_id'], ascending= True)\n",
    "\n",
    "# region_count=[0]*66\n",
    "# for i in range(66):                 #-------storing each regions number of rows in the corresponding index, index 0 = region 1\n",
    "#     region_count[i] = final[final['start_region_id'] == i].shape[0]\n",
    "#     #region_count[i] = final['start_region_id'].value_counts()[i]\n",
    "\n",
    "# print(final)\n",
    "\n",
    "# i=0\n",
    "# for k in region_count:\n",
    "#     print(\"For region: \",i)\n",
    "#     print(\"number of rows: \",k)\n",
    "#     for j in range(k):\n",
    "#         print(\"indexing inside: \",j)\n",
    "#     i+=1\n",
    "\n",
    "# df_sorted.to_csv('mydata.csv', index=False)\n",
    "# print(df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if correcting merged and the hash values are correctly changed to cluster ID\n",
    "\n",
    "#---------example row\n",
    "# 9a864e958859b506f5f8bee9d8dfff17\torderID\n",
    "# a323121d71cd5247f38a4848c2039cb1\tdriverID\n",
    "# b9bd961ee676441d64c8748aa18efcda\tpassengerid\n",
    "# b05379ac3f9b7d99370d443cfd5dcc28\tstartregion\n",
    "# 52d7b69796362a8ed1691a6cc02ddde4   \tdestregion\n",
    "# 45.0\t\t\t\t\tprice\n",
    "# 2016-01-01 00:00:03\t\t\ttimestamp\n",
    "\n",
    "result = train_cluster_map.loc[train_cluster_map['region_hash'] == '52d7b69796362a8ed1691a6cc02ddde4']\n",
    "print(result) \n",
    "result = final.loc[final['order_id'] == '9a864e958859b506f5f8bee9d8dfff17']\n",
    "print(result)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blackconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
