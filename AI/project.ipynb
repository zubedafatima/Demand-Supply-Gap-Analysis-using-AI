{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "#loading test weather data\n",
    "# List all the weather data files\n",
    "file_pattern = 'test_set\\weather_data\\weather_data_*'\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# Read the files and store them in a list of DataFrames\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, sep='\\t', header=None, names=['timestamp', 'weather', 'temperature', 'pollution'])\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all the DataFrames into a single DataFrame\n",
    "test_weather_data=None\n",
    "if dataframes:\n",
    "    test_weather_data = pd.concat(dataframes)\n",
    "else:\n",
    "    exit('No data files found.')\n",
    "\n",
    "# Convert the 'timestamp' column to a datetime object\n",
    "test_weather_data['timestamp'] = pd.to_datetime(test_weather_data['timestamp'])\n",
    "\n",
    "# Reset the index and sort the DataFrame by the timestamp\n",
    "test_weather_data = test_weather_data.sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "print(test_weather_data.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the training weather data\n",
    "file_pattern = 'training_data\\\\weather_data\\\\weather_data_*'\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# Read the files and store them in a list of DataFrames\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, sep='\\t', header=None, names=['timestamp', 'weather', 'temperature', 'pollution'])\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all the DataFrames into a single DataFrame\n",
    "train_weather_data=None\n",
    "if dataframes:\n",
    "    train_weather_data = pd.concat(dataframes)\n",
    "else:\n",
    "    exit('No data files found.')\n",
    "\n",
    "# Convert the 'timestamp' column to a datetime object\n",
    "train_weather_data['timestamp'] = pd.to_datetime(train_weather_data['timestamp'])\n",
    "\n",
    "# Reset the index and sort the DataFrame by the timestamp\n",
    "train_weather_data = train_weather_data.sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "print(train_weather_data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "# loading the test order data\n",
    "# List all the order data files\n",
    "file_pattern = 'test_set/order_data/test_order_data_*'\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# Read the files and store them in a list of DataFrames\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, header=None, names=['order_id', 'driver_id', 'passenger_id', 'start_region_hash', 'Time'])\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all the DataFrames into a single DataFrame\n",
    "test_order_data = None\n",
    "if dataframes:\n",
    "    test_order_data = pd.concat(dataframes)\n",
    "else:\n",
    "    exit('No data files found.')\n",
    "\n",
    "# Convert the 'Time' column to a datetime object\n",
    "test_order_data['Time'] = pd.to_datetime(test_order_data['Time'])\n",
    "\n",
    "# Reset the index and sort the DataFrame by the timestamp\n",
    "test_order_data = test_order_data.sort_values(by='Time').reset_index(drop=True)\n",
    "\n",
    "print(test_order_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the training order data\n",
    "# read training data\n",
    "# List all the training data files\n",
    "file_pattern = 'training_data\\\\order_data\\\\order_data_*'\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# Read the files and store them in a list of DataFrames\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, header=None, sep='\\t', names=['order_id', 'driver_id', 'passenger_id', 'start_region_hash', 'dest_region_hash', 'Price', 'Time'])\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all the DataFrames into a single DataFrame\n",
    "training_order_data = None\n",
    "if dataframes:\n",
    "    training_order_data = pd.concat(dataframes)\n",
    "else:\n",
    "    exit('No data files found.')\n",
    "\n",
    "# Convert the 'timestamp' column to a datetime object\n",
    "training_order_data['Time'] = pd.to_datetime(training_order_data['Time'])\n",
    "\n",
    "# Reset the index and sort the DataFrame by the timestamp\n",
    "training_order_data = training_order_data.sort_values(by='Time').reset_index(drop=True)\n",
    "\n",
    "print(training_order_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test cluster map\n",
    "test_cluster_map = pd.read_csv('test_set\\\\cluster_map\\\\cluster_map', sep='\\t', header=None, names=['region_hash', 'cluster_id'])\n",
    "print(test_cluster_map.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training cluster map\n",
    "train_cluster_map = pd.read_csv('training_data\\\\cluster_map\\\\cluster_map', sep='\\t', header=None, names=['region_hash', 'cluster_id'])\n",
    "print(train_cluster_map.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load test poi data\n",
    "def read_file(filename):\n",
    "    region_data = {}\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            items = line.strip().split('\\t')\n",
    "            region_hash = items[0]\n",
    "            values = items[1:]\n",
    "            region_data[region_hash] = values\n",
    "    return region_data\n",
    "\n",
    "test_poi_data = read_file('test_set\\\\poi_data\\\\poi_data')\n",
    "print(test_poi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training poi data\n",
    "train_poi_data = read_file('training_data\\\\poi_data\\\\poi_data')\n",
    "print(train_poi_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# orderdummy= pd.read_csv('D:\\Python-Project\\AI\\dummyorder.txt', delimiter='\\t')\n",
    "# weatherdummy= pd.read_csv('D:\\Python-Project\\AI\\dummyweather.txt', delimiter='\\t')\n",
    "\n",
    "\n",
    "#---------merging weather data and order data on time\n",
    "merged_df = pd.merge(train_weather_data, training_order_data, left_on='timestamp', right_on='Time',how='right')\n",
    "merged_df = merged_df.drop(columns=['timestamp'])\n",
    "\n",
    "# -------------------Dummy data\n",
    "# merged_df = pd.merge(weatherdummy, orderdummy, left_on='timestamp', right_on='Time',how='outer')\n",
    "# merged_df = merged_df.drop(columns=['timestamp'])\n",
    "\n",
    "\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_category(value):\n",
    "  num = value.split(':')[0]\n",
    "                #-------Storing that value in our set\n",
    "        #print(num)\n",
    "  if('#' in num):\n",
    "      val=num.split('#')                  #-------getting the sub category only instead of main\n",
    "      val=(val[1])\n",
    "  else:\n",
    "      val=num\n",
    "  return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< Updated upstream
    "# def check_facilities(vals, df, idx):\n",
    "#     for f in vals:\n",
    "#         value = split_category(f)\n",
    "#         # col_num = int(value)\n",
    "#         # if col_num in df.columns[:11]:\n",
    "#         #     df.iat[idx, col_num] = 1\n",
    "#         if int(value) in df.columns:\n",
    "#             df.loc[idx, str(value)] = 1\n",
    "def check_facilities(row, train_poi_data):\n",
    "    facilities = train_poi_data.get(row['start_region_hash'], [])\n",
    "\n",
    "    for facility in facilities:\n",
    "        value = split_category(facility)\n",
    "        if str(value) in row.index:\n",
    "            row[str(value)] = 1\n",
    "\n",
    "    return row"
=======
    "def check_facilities(vals,df,idx):\n",
    "  for f in vals:\n",
    "    value=split_category(f)\n",
    "    k=0\n",
    "    for col in df.columns:\n",
    "    # if the column name matches the value\n",
    "        if(k > 10):\n",
    "          if int(col) == int(value):\n",
    "            # change the value of a specific row (e.g. row 0)\n",
    "            \n",
    "            df.loc[idx, col] = 1\n",
    "        k+=1\n",
    "      "
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------Extracting the total unique facilities provided over all from given poin\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# #-------Creating a set as it stores unique values\n",
    "# facilities = set()\n",
    "\n",
    "# #-------Looping over our poid data dictionary\n",
    "# for key, values in train_poi_data.items():\n",
    "#         #------Looping over the list of facilities for a specific region\n",
    "#     for value in values:\n",
    "#                 #-------splitting on semi colon to geth the facility type instead of the number of facility\n",
    "#         val=split_category(value)\n",
    "#         facilities.add(int(val))        #converting to int and storing so easier to sort\n",
    "\n",
    "# sorted_values = sorted(list(facilities))\n",
    "\n",
    "# for col in sorted_values:\n",
    "#         merged_df[col] = 0\n",
    "\n",
    "# #print(sorted_values)\n",
    "# # print(len(sorted_values))\n",
    "# print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      weather  temperature  pollution                          order_id  \\\n",
      "536       NaN          NaN        NaN  b2bb6590661ef7f73d72a162ef64ad22   \n",
      "936       NaN          NaN        NaN  e480019eb193450485dfe6440287c7de   \n",
      "1140      NaN          NaN        NaN  8d4dea8a65af412d8866dd2a5d6354a9   \n",
      "1740      2.0          1.0       53.0  c82100736ccab5f18f1d7741f4157e15   \n",
      "1953      NaN          NaN        NaN  fe47e17e2b8b1d7d92d883b35f446dbb   \n",
      "...       ...          ...        ...                               ...   \n",
      "1090      NaN          NaN        NaN                               NaN   \n",
      "1101      NaN          NaN        NaN                               NaN   \n",
      "1052      NaN          NaN        NaN                               NaN   \n",
      "1103      NaN          NaN        NaN                               NaN   \n",
      "1076      NaN          NaN        NaN                               NaN   \n",
      "\n",
      "                             driver_id                      passenger_id  \\\n",
      "536   8fcbeb749234d33b394ced1473f27ac0  f4d9c3f329d12eae80ae192cd5aa319a   \n",
      "936   59b85ea673b18748ad3a51cbbce95f50  99a2cc0cdff7a0505306094e63c9dab6   \n",
      "1140  4658e925eec1c9a03ff3f7148df31ce2  56018323b921dd2c5444f98fb45509de   \n",
      "1740  2a6befa33e5d5bbdb64e0d951f59b7cf  8728e536cda7a70b7100e0f00b3cd2a8   \n",
      "1953  239a41a9e919a6a5284ce071a67a022f  b41f4eee8a64ee9cb395e824076fe47f   \n",
      "...                                ...                               ...   \n",
      "1090                               NaN                               NaN   \n",
      "1101                               NaN                               NaN   \n",
      "1052                               NaN                               NaN   \n",
      "1103                               NaN                               NaN   \n",
      "1076                               NaN                               NaN   \n",
      "\n",
      "                     start_region_hash                  dest_region_hash  \\\n",
      "536   fc34648599753c9e74ab238e9a4a07ad  fc34648599753c9e74ab238e9a4a07ad   \n",
      "936   364bf755f9b270f0f9141d1a61de43ee  364bf755f9b270f0f9141d1a61de43ee   \n",
      "1140  87285a66236346350541b8815c5fae94  87285a66236346350541b8815c5fae94   \n",
      "1740  dd8d3b9665536d6e05b29c2648c0e69a  a5609739c6b5c2719a3752327c5e33a7   \n",
      "1953  bf44d327f0232325c6d5280926d7b37d  bf44d327f0232325c6d5280926d7b37d   \n",
      "...                                ...                               ...   \n",
      "1090                               NaN                               NaN   \n",
      "1101                               NaN                               NaN   \n",
      "1052                               NaN                               NaN   \n",
      "1103                               NaN                               NaN   \n",
      "1076                               NaN                               NaN   \n",
      "\n",
      "      Price                Time  ...    6    9   12   13   14   16   17   18  \\\n",
      "536     1.0 2016-01-21 21:16:03  ...  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0   \n",
      "936     1.0 2016-01-21 10:28:20  ...  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0   \n",
      "1140    1.0 2016-01-21 13:05:40  ...  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0   \n",
      "1740    8.1 2016-01-21 22:20:37  ...  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0   \n",
      "1953    1.0 2016-01-21 15:53:18  ...  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0   \n",
      "...     ...                 ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "1090    NaN                 NaT  ...  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0   \n",
      "1101    NaN                 NaT  ...  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0   \n",
      "1052    NaN                 NaT  ...  1.0  1.0  1.0  NaN  1.0  1.0  1.0  1.0   \n",
      "1103    NaN                 NaT  ...  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0   \n",
      "1076    NaN                 NaT  ...  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0   \n",
      "\n",
      "       15   19  \n",
      "536   1.0  1.0  \n",
      "936   1.0  1.0  \n",
      "1140  1.0  1.0  \n",
      "1740  1.0  1.0  \n",
      "1953  1.0  1.0  \n",
      "...   ...  ...  \n",
      "1090  1.0  1.0  \n",
      "1101  1.0  1.0  \n",
      "1052  1.0  1.0  \n",
      "1103  1.0  1.0  \n",
      "1076  1.0  1.0  \n",
      "\n",
      "[1172 rows x 58 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming train_poi_data is a dictionary with start_region_hash as key and a list of facilities as value\n",
    "# Assuming merged_df is a DataFrame with 'start_region_hash' column\n",
    "\n",
    "# Get all unique facilities from the train_poi_data dictionary\n",
    "unique_facilities = set()\n",
    "for facilities in train_poi_data.values():\n",
    "    for facility in facilities:\n",
    "        unique_facilities.add(split_category(facility))\n",
    "\n",
    "# Ensure that all unique facilities have columns in merged_df, initialized with 0\n",
    "for facility in unique_facilities:\n",
    "    if str(facility) not in merged_df.columns:\n",
    "        merged_df[str(facility)] = 0\n",
    "\n",
    "# Update the merged_df using the check_facilities function\n",
    "merged_df = merged_df.apply(lambda row: check_facilities(row, train_poi_data), axis=1)\n",
    "\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fc34648599753c9e74ab238e9a4a07ad': 1076, '364bf755f9b270f0f9141d1a61de43ee': 1077, '87285a66236346350541b8815c5fae94': 1072, 'dd8d3b9665536d6e05b29c2648c0e69a': 1082, 'bf44d327f0232325c6d5280926d7b37d': 1059, '74c1c25f4b283fa74a5514307b0d0278': 1087, 'de092beab9305613aca8f79d7d7224e7': 1031, '3a43dcdff3c0b66b1acb1644ff055f9d': 1070, 'd4ec2125aff74eded207d2d915ef682f': 1090, '82cc4851f9e4faa4e54309f8bb73fd7c': 1101, '90c5a34f06ac86aee0fd70e2adce7d8a': 1095, '52e56004d92b8c74d53e1e42699cba6f': 1092, '1ecbb52d73c522f184a6fc53128b1ea1': 1075, 'f2c8c4bb99e6377d21de71275afd6cd2': 1071, '2407d482f0ffa22a947068f2551fe62c': 1105, 'b26a240205c852804ff8758628c0a86a': 1094, '4725c39a5e5f4c188d382da3910b3f3f': 1102, '1cbfbdd079ef93e74405c53fcfff8567': 1057, '1afd7afbc81ecc1b13886a569d869e8a': 1097, 'b05379ac3f9b7d99370d443cfd5dcc28': 1103, '929ec6c160e6f52c20a4217c7978f681': 1088, '62afaf3288e236b389af9cfdc5206415': 1093, 'c4ec24e0a58ebedaa1661e5c09e47bb5': 1073, 'ca064c2682ca48c6a21de012e87c0df5': 1074, '2920ece99323b4c111d6f9affc7ea034': 1099, '3e12208dd0be281c92a6ab57d9a6fb32': 1078, 'd524868ce69cb9db10fc5af177fb9423': 1061, 'f47f35242ed40655814bc086d7514046': 1058, '44c097b7bd219d104050abbafe51bd49': 1083, '91690261186ae5bee8f83808ea1e4a01': 1100, 'ba32abfc048219e933bee869741da911': 1085, '38d5ad2d22b61109fd8e7b43cd0e8901': 1098, 'f9280c5dab6910ed44e518248048b9fe': 1081, 'b702e920dcd2765e624dc1ce3a770512': 1096, 'a814069db8d32f0fa6e188f41059c6e1': 1079, 'a5609739c6b5c2719a3752327c5e33a7': 1080, '4f8d81b5c31af5d1ba579a65ddc8a5cb': 1046, '693a21b16653871bbd455403da5412b4': 1091, 'd05052b4bda7662a084f235e880f50fa': 1104, '73ff8ef735e1d68f0cdcbb84d788f2b6': 1063, 'fff4e8465d1e12621bc361276b6217cf': 1052, '2301bc920194c95cf0c7486e5675243c': 1086, '52d7b69796362a8ed1691a6cc02ddde4': 1064, '52a4e8aaa12f70020e889aed8fd5ddbc': 1089, '8bb37d24db1ad665e706c2655d9c4c72': 1050, '2350be163432e42270d2670cb3c02f80': 1066, '4b7f6f4e2bf237b6cc58f57142bea5c0': 1084, '307afa4120c590b3a46cf4ff5415608a': 1069, '825a21aa308dea206adb49c4b77c7805': 1042, '74ec84f1cf75cf89ae176c8c6ceec5ba': 1048, '0a5fef95db34383403d11cb6af937309': 1008, '8316146a6f78cc6d9f113f0390859417': 1030, 'd5cb17978de290c56e84c9cf97e63186': 1032, '445ff793ebd3477d4a2e0b36b2db9271': 1037, '08232402614a9b48895cc3d0aeb0e9f2': 1039, '49ac89aa860c27e26c0836cb8dab2df2': 1041, '08f5b445ec6b29deba62e6fd8b0325a6': 1045, '7f84bdfc2b6d4541e1f6c0a3349e0251': 1047, 'a735449c5c09df639c35a7d61fad3ee5': 1049, '4f4041f7db0c7f69892d9b74c1a7efa1': 1053, 'cb6041cc08444746caf6039d8b9e43cb': 1054, '1c60154546102e6525f68cb4f31e0657': 1055, '4b9e4cf2fbdc8281b8a1f9f12b80ce4d': 1056, '825c426141df01d38c1b9e9c5330bdac': 1060, 'c9f855e3e13480aad0af64b418e810c3': 1065, '58c7a4888306d8ff3a641d1c0feccbe3': 1067, nan: 1171}\n"
     ]
    }
   ],
   "source": [
    "# start_region_map = {region: idx for idx, region in enumerate(merged_df['start_region_hash'])}\n",
    "# print(start_region_map)\n",
    "# # Iterate over train_poi_data dictionary and set facility values in merged_df dataframe\n",
    "# for key, values in train_poi_data.items():\n",
    "#     start_idx = start_region_map.get(key)\n",
    "#     if start_idx is not None:\n",
    "#         check_facilities(values, merged_df, start_idx)\n",
    "# merged_df=merged_df[merged_df.eq(1).any(axis=1)]\n",
    "\n",
    "for key, values in train_poi_data.items():\n",
    "  # print(key,values)\n",
    "  for index,row in merged_df.iterrows():\n",
    "    if(key == row['start_region_hash']):\n",
    "        check_facilities(values,merged_df,index)\n",
<<<<<<< Updated upstream
    "# # merged_df.to_csv('mydata.csv', index=False)\n",
=======
    "        \n",
    "merged_df.to_csv('semi_final.csv', index=False)\n",
>>>>>>> Stashed changes
    "# print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(merged_df)\n",
    "\n",
    "#---------merging the merged data with cluster map to replace the region hash value with its cluster ID value\n",
    "final = pd.merge(merged_df, train_cluster_map, left_on='start_region_hash', right_on='region_hash')\n",
    "final = final.drop(columns=['region_hash','start_region_hash'])\n",
    "final = final.rename(columns={'cluster_id': 'start_region_id'})\n",
    "\n",
    "final = pd.merge(final, train_cluster_map, left_on='dest_region_hash', right_on='region_hash')\n",
    "final = final.drop(columns=['region_hash','dest_region_hash'])\n",
    "final = final.rename(columns={'cluster_id': 'dest_region_id'})\n",
    "\n",
    "\n",
    "#---------extracting day of the week from the given time stamp\n",
<<<<<<< Updated upstream
    "# final['Time']=pd.to_datetime(final['Time'])\n",
    "\n",
=======
    "final['Time']=pd.to_datetime(final['Time'])\n",
>>>>>>> Stashed changes
    "final['day']=(final['Time'].dt.day_name())\n",
    "\n",
    "# print(\"column count: \",final.shape[1])\n",
    "# print(\"row count\",final.count())\n",
    "#print(final.head())\n",
    "\n",
    "#-------------Sorting the table first according to region then within those regions, sorting according to time \n",
    "sorted_final = final.sort_values(by=['start_region_id', 'Time'])\n",
    "\n",
<<<<<<< Updated upstream
    "print(sorted_final)"
=======
    "# print(sorted_final)"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------Finding the Demand-supply gap for each ten minutes\n",
    "time_slot= final.copy(deep=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------ignore\n",
    "\n",
    "# df_sorted = final.sort_values(by=['start_region_id'], ascending= True)\n",
    "\n",
    "# region_count=[0]*66\n",
    "# for i in range(66):                 #-------storing each regions number of rows in the corresponding index, index 0 = region 1\n",
    "#     region_count[i] = final[final['start_region_id'] == i].shape[0]\n",
    "#     #region_count[i] = final['start_region_id'].value_counts()[i]\n",
    "\n",
    "# print(final)\n",
    "\n",
    "# i=0\n",
    "# for k in region_count:\n",
    "#     print(\"For region: \",i)\n",
    "#     print(\"number of rows: \",k)\n",
    "#     for j in range(k):\n",
    "#         print(\"indexing inside: \",j)\n",
    "#     i+=1\n",
    "\n",
    "# df_sorted.to_csv('mydata.csv', index=False)\n",
    "# print(df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if correcting merged and the hash values are correctly changed to cluster ID\n",
    "\n",
    "#---------example row\n",
    "# 9a864e958859b506f5f8bee9d8dfff17\torderID\n",
    "# a323121d71cd5247f38a4848c2039cb1\tdriverID\n",
    "# b9bd961ee676441d64c8748aa18efcda\tpassengerid\n",
    "# b05379ac3f9b7d99370d443cfd5dcc28\tstartregion\n",
    "# 52d7b69796362a8ed1691a6cc02ddde4   \tdestregion\n",
    "# 45.0\t\t\t\t\tprice\n",
    "# 2016-01-01 00:00:03\t\t\ttimestamp\n",
    "\n",
    "result = train_cluster_map.loc[train_cluster_map['region_hash'] == '52d7b69796362a8ed1691a6cc02ddde4']\n",
    "print(result) \n",
    "result = final.loc[final['order_id'] == '9a864e958859b506f5f8bee9d8dfff17']\n",
    "print(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create a sample dataframe with datetime column and values column\n",
    "df = pd.DataFrame({'datetime': pd.date_range('2022-01-01 00:00:00', '2022-01-01 01:00:00', freq='1min'), 'values': range(61)})\n",
    "# set the datetime column as index\n",
    "print(df)\n",
    "df = df.set_index('datetime')\n",
    "\n",
    "# resample to 10-minute intervals and fill missing values with interpolation\n",
    "df_10min = df.resample('10T').interpolate()\n",
    "\n",
    "# print the result\n",
    "print(df_10min)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blackconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
