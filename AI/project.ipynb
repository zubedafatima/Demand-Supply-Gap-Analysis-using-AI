{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "#loading test weather data\n",
    "# List all the weather data files\n",
    "file_pattern = 'test_set\\weather_data\\weather_data_*'\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# Read the files and store them in a list of DataFrames\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, sep='\\t', header=None, names=['timestamp', 'weather', 'temperature', 'pollution'])\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all the DataFrames into a single DataFrame\n",
    "test_weather_data=None\n",
    "if dataframes:\n",
    "    test_weather_data = pd.concat(dataframes)\n",
    "else:\n",
    "    exit('No data files found.')\n",
    "\n",
    "# Convert the 'timestamp' column to a datetime object\n",
    "test_weather_data['timestamp'] = pd.to_datetime(test_weather_data['timestamp'])\n",
    "\n",
    "# Reset the index and sort the DataFrame by the timestamp\n",
    "test_weather_data = test_weather_data.sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "print(test_weather_data.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the training weather data\n",
    "file_pattern = 'training_data\\\\weather_data\\\\weather_data_*'\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# Read the files and store them in a list of DataFrames\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, sep='\\t', header=None, names=['timestamp', 'weather', 'temperature', 'pollution'])\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all the DataFrames into a single DataFrame\n",
    "train_weather_data=None\n",
    "if dataframes:\n",
    "    train_weather_data = pd.concat(dataframes)\n",
    "else:\n",
    "    exit('No data files found.')\n",
    "\n",
    "# Convert the 'timestamp' column to a datetime object\n",
    "train_weather_data['timestamp'] = pd.to_datetime(train_weather_data['timestamp'])\n",
    "\n",
    "# Reset the index and sort the DataFrame by the timestamp\n",
    "train_weather_data = train_weather_data.sort_values(by='timestamp').reset_index(drop=True)\n",
    "\n",
    "print(train_weather_data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "# loading the test order data\n",
    "# List all the order data files\n",
    "file_pattern = 'test_set/order_data/test_order_data_*'\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# Read the files and store them in a list of DataFrames\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, header=None, names=['order_id', 'driver_id', 'passenger_id', 'start_region_hash', 'Time'])\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all the DataFrames into a single DataFrame\n",
    "test_order_data = None\n",
    "if dataframes:\n",
    "    test_order_data = pd.concat(dataframes)\n",
    "else:\n",
    "    exit('No data files found.')\n",
    "\n",
    "# Convert the 'Time' column to a datetime object\n",
    "test_order_data['Time'] = pd.to_datetime(test_order_data['Time'])\n",
    "\n",
    "# Reset the index and sort the DataFrame by the timestamp\n",
    "test_order_data = test_order_data.sort_values(by='Time').reset_index(drop=True)\n",
    "\n",
    "print(test_order_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the training order data\n",
    "# read training data\n",
    "# List all the training data files\n",
    "file_pattern = 'training_data\\\\order_data\\\\order_data_*'\n",
    "files = glob.glob(file_pattern)\n",
    "\n",
    "# Read the files and store them in a list of DataFrames\n",
    "dataframes = []\n",
    "for file in files:\n",
    "    df = pd.read_csv(file, header=None, sep='\\t', names=['order_id', 'driver_id', 'passenger_id', 'start_region_hash', 'dest_region_hash', 'Price', 'Time'])\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all the DataFrames into a single DataFrame\n",
    "training_order_data = None\n",
    "if dataframes:\n",
    "    training_order_data = pd.concat(dataframes)\n",
    "else:\n",
    "    exit('No data files found.')\n",
    "\n",
    "# Convert the 'timestamp' column to a datetime object\n",
    "training_order_data['Time'] = pd.to_datetime(training_order_data['Time'])\n",
    "\n",
    "# Reset the index and sort the DataFrame by the timestamp\n",
    "training_order_data = training_order_data.sort_values(by='Time').reset_index(drop=True)\n",
    "\n",
    "print(training_order_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test cluster map\n",
    "test_cluster_map = pd.read_csv('test_set\\\\cluster_map\\\\cluster_map', sep='\\t', header=None, names=['region_hash', 'cluster_id'])\n",
    "print(test_cluster_map.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training cluster map\n",
    "train_cluster_map = pd.read_csv('training_data\\\\cluster_map\\\\cluster_map', sep='\\t', header=None, names=['region_hash', 'cluster_id'])\n",
    "print(train_cluster_map.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load test poi data\n",
    "def read_file(filename):\n",
    "    region_data = {}\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            items = line.strip().split('\\t')\n",
    "            region_hash = items[0]\n",
    "            values = items[1:]\n",
    "            region_data[region_hash] = values\n",
    "    return region_data\n",
    "\n",
    "test_poi_data = read_file('test_set\\\\poi_data\\\\poi_data')\n",
    "print(test_poi_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training poi data\n",
    "train_poi_data = read_file('training_data\\\\poi_data\\\\poi_data')\n",
    "print(train_poi_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #-------------------Dummy data\n",
    "\n",
    "# orderdummy= pd.read_csv('D:\\Python-Project\\AI\\dummyorder.txt', delimiter='\\t')\n",
    "# weatherdummy= pd.read_csv('D:\\Python-Project\\AI\\dummyweather.txt', delimiter='\\t')\n",
    "\n",
    "# merged_df = pd.merge(weatherdummy, orderdummy, left_on='timestamp', right_on='Time',how='outer')\n",
    "# merged_df = merged_df.drop(columns=['timestamp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_category(value):\n",
    "  num = value.split(':')[0]\n",
    "  #-------Storing that value in our set\n",
    "    #print(num)\n",
    "  if('#' in num):\n",
    "      val=num.split('#')                  #-------getting the sub category only instead of main\n",
    "      val=(val[1])\n",
    "  else:\n",
    "      val=num\n",
    "  \n",
    "  return val\n",
    "\n",
    "def split_count(value):\n",
    "  num = value.split(':')[1]\n",
    "  return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Denormalizing(order_data,weather_data,poi_data,cluster_data,type):\n",
    "        merged_df=order_data.copy(deep=True)\n",
    "        region_count={}\n",
    "    #-----------------------counting the total facilities in each region\n",
    "        for key, values in poi_data.items():      \n",
    "            count=0\n",
    "    #------Looping over the list of facilities for a specific region\n",
    "            for value in values:\n",
    "            #-------splitting on semi colon to geth the facility type instead of the number of facility\n",
    "                    val=split_count(value)\n",
    "                    count=int(val)+count\n",
    "            #print(\"Region: count: \",key,count)\n",
    "            region_count.update({key: count})\n",
    "\n",
    "        merged_df['Facilities'] = merged_df['start_region_hash'].map(region_count)\n",
    "    #print(merged_df)\n",
    "\n",
    "        final = pd.merge(merged_df,cluster_data, left_on='start_region_hash', right_on='region_hash')\n",
    "        final = final.drop(columns=['region_hash','start_region_hash'])\n",
    "        final = final.rename(columns={'cluster_id': 'start_region_id'})\n",
    "\n",
    "        if(type == 'train'):    #because test set doesnt have destionation hash\n",
    "            final = pd.merge(final, cluster_data, left_on='dest_region_hash', right_on='region_hash')\n",
    "            final = final.drop(columns=['region_hash','dest_region_hash'])\n",
    "            final = final.rename(columns={'cluster_id': 'dest_region_id'})\n",
    "\n",
    "\n",
    "        #---------extracting day of the week from the given time stamp\n",
    "        final['Time']=pd.to_datetime(final['Time'])\n",
    "        final['day']=(final['Time'].dt.day_name())\n",
    "\n",
    "        # print(\"column count: \",final.shape[1])\n",
    "        # print(\"row count\",final.count())\n",
    "        #print(final.head())\n",
    "\n",
    "        #-------------Sorting the table first according to region then within those regions, sorting according to time \n",
    "        final = final.sort_values(by=['start_region_id', 'Time'])\n",
    "\n",
    "        #----------------------finding demand supply gap\n",
    "        #Group by Region and time in 10 minute intervals\n",
    "        training = final.groupby(['start_region_id', pd.Grouper(key='Time', freq='10min')]).agg({'order_id': 'count', 'driver_id': lambda x: x.notnull().sum(),'Facilities':'mean'}).reset_index()\n",
    "\n",
    "        #calculating the demand-gap by finding difference between total orders and given drivers\n",
    "        training['Gap'] = training.apply(lambda row: (row['order_id'] - row['driver_id']), axis=1)\n",
    "        # print(training)\n",
    "\n",
    "        #Grouping weather data into 10 min slots as well\n",
    "        weather = weather_data.groupby(['weather', pd.Grouper(key='timestamp', freq='10min')]).agg({'pollution': 'mean','temperature':'mean',}).reset_index()\n",
    "        # print(weather)\n",
    "\n",
    "        #---------merging weather data and order data on time\n",
    "        Demand_supply = pd.merge(weather, training, left_on='timestamp', right_on='Time',how='right')\n",
    "        Demand_supply = Demand_supply.drop(columns=['timestamp'])\n",
    "\n",
    "        \n",
    "        return Demand_supply\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Training=Denormalizing(training_order_data,train_weather_data,train_poi_data,train_cluster_map,'train')\n",
    "print(Training)\n",
    "Training.to_csv(\"Training.csv\",index=False)\n",
    "Testing=Denormalizing(test_order_data,test_weather_data,test_poi_data,test_cluster_map,'test')\n",
    "print(Testing)\n",
    "Testing.to_csv(\"Testing.csv\",index=False)\n",
    "\n",
    "# #------counting how many rows of each region\n",
    "# counts = Demand_supply ['start_region_id'].value_counts()\n",
    "# region_counts = counts.reset_index().rename(columns={'index': 'region'})\n",
    "# region_counts=region_counts.sort_values(by='region',ascending=True)\n",
    "# print(region_counts)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged_df=training_order_data.copy(deep=True)\n",
    "# region_count={}\n",
    "# #-----------------------counting the total facilities in each region\n",
    "# for key, values in train_poi_data.items():      \n",
    "#         count=0\n",
    "# #------Looping over the list of facilities for a specific region\n",
    "#         for value in values:\n",
    "#         #-------splitting on semi colon to geth the facility type instead of the number of facility\n",
    "#                 val=split_count(value)\n",
    "#                 count=int(val)+count\n",
    "#         #print(\"Region: count: \",key,count)\n",
    "#         region_count.update({key: count})\n",
    "\n",
    "# merged_df['Facilities'] = merged_df['start_region_hash'].map(region_count)\n",
    "# print(merged_df)\n",
    "           \n",
    "\n",
    "\n",
    "# #---------merging the merged data with cluster map to replace the region hash value with its cluster ID value\n",
    "# final = pd.merge(merged_df, train_cluster_map, left_on='start_region_hash', right_on='region_hash')\n",
    "# final = final.drop(columns=['region_hash','start_region_hash'])\n",
    "# final = final.rename(columns={'cluster_id': 'start_region_id'})\n",
    "\n",
    "# final = pd.merge(final, train_cluster_map, left_on='dest_region_hash', right_on='region_hash')\n",
    "# final = final.drop(columns=['region_hash','dest_region_hash'])\n",
    "# final = final.rename(columns={'cluster_id': 'dest_region_id'})\n",
    "\n",
    "\n",
    "# #---------extracting day of the week from the given time stamp\n",
    "# final['Time']=pd.to_datetime(final['Time'])\n",
    "# final['day']=(final['Time'].dt.day_name())\n",
    "\n",
    "# # print(\"column count: \",final.shape[1])\n",
    "# # print(\"row count\",final.count())\n",
    "# #print(final.head())\n",
    "\n",
    "# #-------------Sorting the table first according to region then within those regions, sorting according to time \n",
    "# final = final.sort_values(by=['start_region_id', 'Time'])\n",
    "\n",
    "\n",
    "# #----------------------Finding the Demand-supply gap for each ten minutes\n",
    "# #print(sorted_final)\n",
    "\n",
    "# #Group by Region and time in 10 minute intervals\n",
    "# training = final.groupby(['start_region_id', pd.Grouper(key='Time', freq='10min')]).agg({'order_id': 'count', 'driver_id': lambda x: x.notnull().sum(),'Facilities':'mean'}).reset_index()\n",
    "\n",
    "# #calculating the demand-gap by finding difference between total orders and given drivers\n",
    "# training['Gap'] = training.apply(lambda row: (row['order_id'] - row['driver_id']), axis=1)\n",
    "# # print(training)\n",
    "\n",
    "# #Grouping weather data into 10 min slots as well\n",
    "# weather = train_weather_data.groupby(['weather', pd.Grouper(key='timestamp', freq='10min')]).agg({'pollution': 'mean','temperature':'mean',}).reset_index()\n",
    "# # print(weather)\n",
    "\n",
    "# #---------merging weather data and order data on time\n",
    "# Demand_supply = pd.merge(weather, training, left_on='timestamp', right_on='Time',how='right')\n",
    "# Demand_supply = Demand_supply.drop(columns=['timestamp'])\n",
    "\n",
    "# print(Demand_supply)\n",
    "# Demand_supply.to_csv('Training_Demand_supply.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------ignore\n",
    "\n",
    "# df_sorted = final.sort_values(by=['start_region_id'], ascending= True)\n",
    "\n",
    "# region_count=[0]*66\n",
    "# for i in range(66):                 #-------storing each regions number of rows in the corresponding index, index 0 = region 1\n",
    "#     region_count[i] = final[final['start_region_id'] == i].shape[0]\n",
    "#     #region_count[i] = final['start_region_id'].value_counts()[i]\n",
    "\n",
    "# print(final)\n",
    "\n",
    "# i=0\n",
    "# for k in region_count:\n",
    "#     print(\"For region: \",i)\n",
    "#     print(\"number of rows: \",k)\n",
    "#     for j in range(k):\n",
    "#         print(\"indexing inside: \",j)\n",
    "#     i+=1\n",
    "\n",
    "# df_sorted.to_csv('mydata.csv', index=False)\n",
    "# print(df_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------Extracting the total unique facilities provided over all from given poin\n",
    "\n",
    "# import pandas as pd\n",
    "# #-------Creating a set as it stores unique values\n",
    "# facilities = set()\n",
    "# #-------Looping over our poid data dictionary\n",
    "# for key, values in train_poi_data.items():\n",
    "#         #------Looping over the list of facilities for a specific region\n",
    "#     for value in values:\n",
    "#                 #-------splitting on semi colon to geth the facility type instead of the number of facility\n",
    "#         val=split_category(value)\n",
    "#         facilities.add(int(val))        #converting to int and storing so easier to sort\n",
    "\n",
    "# sorted_values = sorted(list(facilities))\n",
    "\n",
    "# for col in sorted_values:\n",
    "#         merged_df[col] = 0\n",
    "\n",
    "# #print(sorted_values)\n",
    "# #print(len(sorted_values))\n",
    "# #print(merged_df)\n",
    "\n",
    "# def check_facilities(vals, df, idx):\n",
    "#     for f in vals:\n",
    "#         value = split_category(f)\n",
    "#         if int(value) in df.columns:\n",
    "#             df.loc[idx, str(value)] = 1\n",
    "# # ---------for each region that has the facility replace the 0 with 1\n",
    "# # ----------------------to do, iterate over the data frame and for the regions of order put 1 \n",
    "# #for the facilities in that region(either start or destination)\n",
    "# for key, values in train_poi_data.items():\n",
    "#   # print(key,values)\n",
    "#   for index,row in merged_df.iterrows():\n",
    "#     if(key == row['start_region_hash']):\n",
    "#         check_facilities(values,merged_df,index)\n",
    "\n",
    "\n",
    "# print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if correcting merged and the hash values are correctly changed to cluster ID\n",
    "\n",
    "#---------example row\n",
    "# 9a864e958859b506f5f8bee9d8dfff17\torderID\n",
    "# a323121d71cd5247f38a4848c2039cb1\tdriverID\n",
    "# b9bd961ee676441d64c8748aa18efcda\tpassengerid\n",
    "# b05379ac3f9b7d99370d443cfd5dcc28\tstartregion\n",
    "# 52d7b69796362a8ed1691a6cc02ddde4   \tdestregion\n",
    "# 45.0\t\t\t\t\tprice\n",
    "# 2016-01-01 00:00:03\t\t\ttimestamp\n",
    "\n",
    "# result = train_cluster_map.loc[train_cluster_map['region_hash'] == '52d7b69796362a8ed1691a6cc02ddde4']\n",
    "# print(result) \n",
    "# result = final.loc[final['order_id'] == '9a864e958859b506f5f8bee9d8dfff17']\n",
    "# print(result)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blackconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
